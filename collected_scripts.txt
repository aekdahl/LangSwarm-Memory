# Total Document Length: 121054 characters


--------------------------------------------------------------------------------
File: tests/__init__.py
--------------------------------------------------------------------------------





--------------------------------------------------------------------------------
File: memory/memory_manager.py
--------------------------------------------------------------------------------

import functools

class MemoryManager:
    def __init__(self, backends=None, **kwargs):
        """
        Initialize MemoryManager with multiple backends.

        Args:
            backends (list): List of backend configurations. Each entry can specify
                            the backend type (e.g., "langchain", "llama_index") and
                            corresponding parameters.
        """
        self.adapters = []
        if backends:
            for backend in backends:
                if backend["type"] == "langchain":
                    self.adapters.append(LangChainAdapter(**backend.get("params", {})))
                elif backend["type"] == "llama_index":
                    self.adapters.append(LlamaIndexAdapter(**backend.get("params", {})))
                else:
                    raise ValueError(f"Unsupported backend: {backend['type']}")

    def add_documents(self, documents):
        for adapter in self.adapters:
            adapter.add_documents(documents)

    def query(self, query, filters=None, sort_key=None, top_k=None):
        """
        Query all backends and aggregate results with deduplication and sorting.
    
        Args:
            query (str): Query string.
            filters (dict): Optional filters for querying.
            sort_key (str): Optional key for sorting results.
            top_k (int): Optional number of top results to return.
    
        Returns:
            list: Deduplicated and sorted query results.
        """
        results = []
        for adapter in self.adapters:
            results.extend(adapter.query(query, filters))
    
        # Deduplicate results by text (or other unique key)
        unique_results = {res["text"]: res for res in results}.values()
    
        # Sort results if a sort key is provided
        if sort_key:
            unique_results = sorted(unique_results, key=lambda x: x.get(sort_key), reverse=True)
    
        # Limit to top_k results if specified
        if top_k:
            unique_results = list(unique_results)[:top_k]
    
        return list(unique_results)


    def delete(self, document_ids):
        for adapter in self.adapters:
            adapter.delete(document_ids)



"""
The SharedMemoryManager will unify and orchestrate memory backends, enabling seamless integration with the LangSwarm ecosystem. Its primary role is to handle shared memory operations across multiple backends and provide a consistent interface for:

Centralized and Federated Memory: Supporting both centralized memory (e.g., for global indices) and federated memory (e.g., agent-specific memory).
Thread-Safe Operations: Ensuring safe concurrent access to shared memory.
Multi-Backend Orchestration: Allowing flexible switching and management of memory backends (e.g., FAISS, Pinecone, Elasticsearch).
"""
class SharedMemoryManager:
    def __init__(self, backend_configs, thread_safe=True):
        """
        Initializes the shared memory manager.

        Args:
            backend_configs (list): List of backend configurations. Each entry specifies the backend type 
                                    (e.g., "faiss", "pinecone") and its parameters.
            thread_safe (bool): Whether to make the manager thread-safe.
        """
        self.backends = self._initialize_backends(backend_configs)
        self.lock = threading.RLock() if thread_safe else None

    def _initialize_backends(self, backend_configs):
        """
        Initializes memory backends based on configurations.

        Args:
            backend_configs (list): List of configurations for each backend.

        Returns:
            list: List of initialized backend instances.
        """
        initialized_backends = []
        for config in backend_configs:
            if config["type"] == "faiss":
                initialized_backends.append(FaissAdapter(**config.get("params", {})))
            elif config["type"] == "pinecone":
                initialized_backends.append(PineconeAdapter(**config.get("params", {})))
            else:
                raise ValueError(f"Unsupported backend type: {config['type']}")
        return initialized_backends

    def _example_usage(self):
        """
        Example usage of the SharedMemoryManager.
        """
        backend_configs = [
            {"type": "faiss", "params": {"dimension": 128}},
            {"type": "pinecone", "params": {"api_key": "your-api-key", "environment": "us-west1"}}
        ]
        manager = SharedMemoryManager(backend_configs)
        documents = [{"text": "Document 1"}, {"text": "Document 2"}]
        manager.add_documents(documents)
        results = manager.query("query text")
        print("Results:", results)

    def _thread_safe(method):
        """Decorator to add thread-safety to methods if enabled."""
        @functools.wraps(method)
        def wrapper(self, *args, **kwargs):
            if hasattr(self, 'lock') and self.lock:  # Ensure instance has a lock attribute
                with self.lock:
                    return method(self, *args, **kwargs)
            return method(self, *args, **kwargs)
        return wrapper

    @_thread_safe
    def add_documents(self, documents):
        for backend in self.backends:
            backend.add_documents(documents)

    @_thread_safe
    def query(self, query_params, deduplicate=True, sort_key=None, sort_reverse=False):
        """
        Query shared memory segments for matching results.

        Args:
            query_params (dict): Parameters to filter the query.
            deduplicate (bool): Whether to deduplicate results. Default is True.
            sort_key (str): Key to sort results by (e.g., 'timestamp').
            sort_reverse (bool): Whether to reverse the sorting order. Default is False.

        Returns:
            list: A list of query results.
        """
        results = []
        
        # Aggregate results from all shared memory segments
        for segment in self._segments:
            results.extend(segment.query(query_params))

        # Deduplicate results if required
        if deduplicate:
            # Assuming each result is hashable (e.g., dictionaries with immutable keys/values)
            results = list({frozenset(item.items()): item for item in results}.values())

        # Sort results if a sort key is provided
        if sort_key:
            try:
                results.sort(key=lambda x: x.get(sort_key), reverse=sort_reverse)
            except KeyError:
                raise ValueError(f"Sort key '{sort_key}' not found in results.")

        return results

    @_thread_safe
    def delete(self, ids):
        for backend in self.backends:
            backend.delete(ids)




--------------------------------------------------------------------------------
File: memory/centralized_index.py
--------------------------------------------------------------------------------

import importlib
from datetime import datetime, timedelta

try:
    from llama_index import GPTSimpleVectorIndex, Document
    LLAMA_INDEX_AVAILABLE = True
except ImportError:
    GPTSimpleVectorIndex = None
    Document = None
    LLAMA_INDEX_AVAILABLE = False


class CentralizedIndex:
    def __init__(self, index_path="memory_index.json", expiration_days=None):
        """
        Centralized index for long-term memory and shared knowledge.

        :param index_path: Path to store the index file.
        :param expiration_days: Number of days before memory fades (optional).
        """
        self.index_path = index_path
        self.expiration_days = expiration_days
        self._indexing_is_available = LLAMA_INDEX_AVAILABLE

        if not LLAMA_INDEX_AVAILABLE:
            self.index = None
            print("LlamaIndex is not installed. Memory indexing features are disabled.")
            return

        # Try to load an existing index or create a new one
        try:
            self.index = GPTSimpleVectorIndex.load_from_disk(index_path)
        except FileNotFoundError:
            self.index = GPTSimpleVectorIndex([])

    @property
    def indexing_is_available(self):
        """Check if indexing is available."""
        return self._indexing_is_available

    def _clean_expired_documents(self):
        """
        Internal method to clean up expired documents.
        """
        if not self.indexing_is_available or self.expiration_days is None:
            return
    
        now = datetime.now()
        valid_documents = []
        for doc in self.index.documents:
            timestamp = doc.extra_info.get("timestamp")
            if timestamp:
                doc_time = datetime.fromisoformat(timestamp)
                if (now - doc_time) <= timedelta(days=self.expiration_days):
                    valid_documents.append(doc)
    
        # Update the index with valid documents only
        self.index = GPTSimpleVectorIndex(valid_documents)
        self.index.save_to_disk(self.index_path)
    
    def _validate_and_normalize_metadata(self, metadata):
        """
        Validates and normalizes metadata.
    
        Args:
            metadata (dict): Metadata dictionary.
    
        Returns:
            dict: Normalized metadata with lowercase keys.
        """
        if not isinstance(metadata, dict):
            raise ValueError("Metadata must be a dictionary.")
    
        return {str(key).lower(): value for key, value in metadata.items()}
    
    def add_documents(self, docs):
        """
        Add documents to the centralized index with metadata validation.
    
        :param docs: List of documents with text and optional metadata.
        """
        if not self.indexing_is_available:
            print("Indexing features are unavailable.")
            return
    
        self._clean_expired_documents()
    
        documents = [
            Document(text=doc["text"], metadata={
                **self._validate_and_normalize_metadata(doc.get("metadata", {})),
                "timestamp": datetime.now().isoformat()  # Add a timestamp to each document
            })
            for doc in docs
        ]
        self.index.insert(documents)
        self.index.save_to_disk(self.index_path)
    
    def query(self, query_text, metadata_filter=None):
        """
        Query the index with metadata validation and filtering.
    
        :param query_text: The text query.
        :param metadata_filter: Dictionary of metadata filters (optional).
        :return: Filtered results based on the query and metadata.
        """
        self._clean_expired_documents()
    
        if not self.indexing_is_available:
            print("Indexing features are unavailable.")
            return []
    
        results = self.index.query(query_text)
    
        # Apply metadata filtering if specified
        if metadata_filter:
            normalized_filter = self._validate_and_normalize_metadata(metadata_filter)
            results = [
                res for res in results
                if all(res.extra_info.get(key) == value for key, value in normalized_filter.items())
            ]
        return results


    def purge_expired_documents(self):
        """
        Remove documents from the index that have exceeded the expiration period.
        """
        self._clean_expired_documents()





class HybridCentralizedIndex:
    def __init__(self, adapters: List[Any] = None):
        self.adapters = adapters or []

    def add_adapter(self, adapter: Any):
        """Add a new adapter to the hybrid index."""
        self.adapters.append(adapter)

    def remove_adapter(self, adapter_name: str):
        """Remove an adapter by name."""
        self.adapters = [a for a in self.adapters if a.__class__.__name__ != adapter_name]

    def query(self, query: str, **kwargs) -> List[Dict[str, Any]]:
        """Query all adapters that support the specified type."""
        query_type = kwargs.get("type", "default")
        results = []

        for adapter in self.adapters:
            capabilities = adapter.capabilities()
            if (query_type == "vector_search" and capabilities.get("vector_search")) or \
               (query_type == "metadata_filtering" and capabilities.get("metadata_filtering")) or \
               query_type == "default":
                try:
                    results.extend(adapter.query(query, **kwargs))
                except Exception as e:
                    print(f"Adapter {adapter.__class__.__name__} failed: {e}")

        return self._deduplicate_and_sort(results)

    def _deduplicate_and_sort(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Deduplicate and sort results by score."""
        unique_results = {res["id"]: res for res in results}  # Deduplicate by ID
        return sorted(unique_results.values(), key=lambda x: x["score"], reverse=True)

"""
# Example Usage
if __name__ == "__main__":
    # Initialize existing adapters
    faiss_adapter = FAISSAdapter(index_path="path/to/faiss")
    es_adapter = ElasticsearchAdapter(host="localhost", port=9200)
    pinecone_adapter = PineconeAdapter(api_key="api_key", environment="us-west")

    # Initialize Hybrid Centralized Index
    hybrid_index = HybridCentralizedIndex([faiss_adapter, es_adapter, pinecone_adapter])

    # Query for vector search
    vector_results = hybrid_index.query("What is AI?", type="vector_search", top_k=5)
    print("Vector Search Results:", vector_results)

    # Query for metadata filtering
    metadata_results = hybrid_index.query("What is AI?", type="metadata_filtering", metadata={"tag": "AI"})
    print("Metadata Filtering Results:", metadata_results)
"""



--------------------------------------------------------------------------------
File: memory/wrappers/thread_safe_adapter.py
--------------------------------------------------------------------------------

import threading

class ThreadSafeAdapter:
    def __init__(self, adapter):
        """
        Wraps an existing adapter to make it thread-safe.

        Args:
            adapter: The adapter to wrap (e.g., FAISSBackend, PineconeBackend, etc.).
        """
        self.adapter = adapter
        self.lock = threading.RLock()

    def add_documents(self, documents):
        """
        Thread-safe method to add documents to the underlying adapter.

        Args:
            documents (list): List of documents to add.

        Raises:
            ValueError: If the input is not a list or documents lack required fields.
        """
        if not isinstance(documents, list):
            raise ValueError("Documents must be provided as a list.")
        for doc in documents:
            if not isinstance(doc, dict) or "text" not in doc:
                raise ValueError("Each document must be a dictionary with at least a 'text' key.")

        with self.lock:
            try:
                self.adapter.add_documents(documents)
            except Exception as e:
                raise RuntimeError(f"Failed to add documents: {e}")

    def query(self, query, top_k=5):
        """
        Thread-safe method to query the underlying adapter.

        Args:
            query (str): Query string.
            top_k (int): Number of top results to retrieve.

        Returns:
            list: Query results.

        Raises:
            ValueError: If query is not a string or top_k is not a positive integer.
            RuntimeError: If querying the adapter fails.
        """
        if not isinstance(query, str):
            raise ValueError("Query must be a string.")
        if not isinstance(top_k, int) or top_k <= 0:
            raise ValueError("top_k must be a positive integer.")

        with self.lock:
            try:
                return self.adapter.query(query, top_k)
            except Exception as e:
                raise RuntimeError(f"Failed to query adapter: {e}")

    def delete(self, ids):
        """
        Thread-safe method to delete documents from the underlying adapter.

        Args:
            ids (list): List of document IDs to delete.

        Raises:
            ValueError: If ids is not a list or is empty.
            RuntimeError: If deletion fails.
        """
        if not isinstance(ids, list) or not ids:
            raise ValueError("IDs must be provided as a non-empty list.")

        with self.lock:
            try:
                self.adapter.delete(ids)
            except Exception as e:
                raise RuntimeError(f"Failed to delete documents: {e}")

    def __getattr__(self, attr):
        """
        Delegate any other methods or attributes to the underlying adapter.

        This allows access to other methods (if needed) without redefining them.

        Args:
            attr (str): Attribute name.

        Returns:
            Any: The corresponding attribute from the adapter.

        Raises:
            AttributeError: If the attribute does not exist in the adapter.
        """
        try:
            return getattr(self.adapter, attr)
        except AttributeError as e:
            raise AttributeError(f"Attribute '{attr}' not found in the underlying adapter: {e}")



--------------------------------------------------------------------------------
File: memory/rerankers/rerank.py
--------------------------------------------------------------------------------

class BaseReranker:
    """
    A base class for all rerankers in LangSwarm.

    Methods:
        rerank(query: str, documents: list) -> list:
            Rerank the provided documents based on the query.
    """
    def rerank(self, query, documents):
        raise NotImplementedError("Subclasses must implement the `rerank` method.")



--------------------------------------------------------------------------------
File: memory/rerankers/hugging_face.py
--------------------------------------------------------------------------------

class HuggingFaceReranker(BaseReranker):
    """
    Reranker using Hugging Face SentenceTransformer models for semantic similarity.

    If the specified model is unavailable, a default model ('all-MiniLM-L6-v2') is used as a fallback.
    """

    DEFAULT_MODEL = "all-MiniLM-L6-v2"  # Default lightweight model for general-purpose reranking

    def __init__(self, model_name=None):
        """
        Initialize the HuggingFaceReranker.

        Args:
            model_name (str): Name of the Hugging Face model to use. Defaults to `DEFAULT_MODEL`.
        """
        from sentence_transformers import SentenceTransformer, util
        self.util = util
        self.model_name = model_name or self.DEFAULT_MODEL

        try:
            self.model = SentenceTransformer(self.model_name)
        except Exception as e:
            print(f"Error loading model '{self.model_name}': {e}")
            print(f"Falling back to default model: '{self.DEFAULT_MODEL}'")
            self.model = SentenceTransformer(self.DEFAULT_MODEL)

    def rerank(self, query, documents):
        """
        Rerank documents based on semantic similarity to the query.

        Args:
            query (str): The query string.
            documents (list): List of documents with 'text' and optional 'metadata'.

        Returns:
            list: Documents sorted by relevance score.
        """
        # Validate inputs
        if not isinstance(query, str):
            raise ValueError("Query must be a string.")
        if not isinstance(documents, list) or not all(isinstance(doc, dict) for doc in documents):
            raise ValueError("Documents must be a list of dictionaries.")

        query_embedding = self.model.encode(query, convert_to_tensor=True)
        results = []
        for doc in documents:
            doc_embedding = self.model.encode(doc["text"], convert_to_tensor=True)
            score = self.util.pytorch_cos_sim(query_embedding, doc_embedding).item()
            results.append({"text": doc["text"], "metadata": doc.get("metadata", {}), "score": score})
        return sorted(results, key=lambda x: x["score"], reverse=True)

    @staticmethod
    def validate_model(model_name):
        """
        Validate if the specified model is available for use.

        Args:
            model_name (str): Name of the Hugging Face model.

        Returns:
            bool: True if the model is available, False otherwise.
        """
        from sentence_transformers import SentenceTransformer
        try:
            SentenceTransformer(model_name)
            return True
        except Exception as e:
            print(f"Validation failed for model '{model_name}': {e}")
            return False



class HuggingFaceSemanticReranker(BaseReranker):
    """
    Reranks documents based on semantic similarity using Hugging Face models.

    Supports pre-trained and fine-tuned models for specific domains.

    Pre-Trained and Domain-Specific Models:
    ---------------------------------------
    General Models:
        - "all-MiniLM-L6-v2": Lightweight, general-purpose semantic similarity.
        - "all-mpnet-base-v2": High-performance general-purpose model.
    
    Domain-Specific Models:
        - Biomedical:
            - "sci-bert/scibert-scivocab-uncased": Optimized for scientific research.
            - "biobert-v1.1": Fine-tuned for biomedical text.
        - Legal:
            - "nlpaueb/legal-bert-base-uncased": Designed for legal documents.
        - Finance:
            - "finbert": Fine-tuned for financial data.
        - Customer Support:
            - "paraphrase-multilingual-mpnet-base-v2": Multilingual customer query handling.

    Usage Example:
    --------------
    1. Initialize the reranker:
        reranker = HuggingFaceSemanticReranker(model_name="biobert-v1.1")

    2. Provide query and documents:
        query = "What are the effects of this drug on the immune system?"
        documents = [
            {"text": "This drug enhances immune response in patients with cancer."},
            {"text": "The medication targets immune cells to reduce inflammation."},
        ]

    3. Perform reranking:
        results = reranker.rerank(query, documents)

    Returns:
        A list of documents sorted by relevance score.
    """
    def __init__(self, model_name="all-MiniLM-L6-v2"):
        """
        Initialize the reranker with a Hugging Face model.

        Args:
            model_name (str): Name of the Hugging Face model to use.
        """
        from sentence_transformers import SentenceTransformer, util
        self.model = SentenceTransformer(model_name)

    def rerank(self, query, documents):
        """
        Rerank documents based on semantic similarity to the query.

        Args:
            query (str): The query string.
            documents (list): List of documents with 'text' and optional 'metadata'.

        Returns:
            list: Documents sorted by relevance score.
        """
        query_embedding = self.model.encode(query, convert_to_tensor=True)
        results = []
        for doc in documents:
            doc_embedding = self.model.encode(doc["text"], convert_to_tensor=True)
            score = util.pytorch_cos_sim(query_embedding, doc_embedding).item()
            results.append({"text": doc["text"], "metadata": doc.get("metadata", {}), "score": score})
        return sorted(results, key=lambda x: x["score"], reverse=True)


class HuggingFaceDPRReranker(BaseReranker):
    def __init__(self, model_name="facebook/dpr-question_encoder-single-nq-base"):
        from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder
        self.query_model = DPRQuestionEncoder.from_pretrained(model_name)
        self.query_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(model_name)
        self.context_model = DPRContextEncoder.from_pretrained(model_name)

    def rerank(self, query, documents):
        """
        Rerank documents using Dense Passage Retrieval (DPR).
        """
        query_inputs = self.query_tokenizer(query, return_tensors="pt")
        query_embedding = self.query_model(**query_inputs).pooler_output

        results = []
        for doc in documents:
            context_inputs = self.query_tokenizer(doc["text"], return_tensors="pt")
            context_embedding = self.context_model(**context_inputs).pooler_output
            score = (query_embedding * context_embedding).sum().item()
            results.append({"text": doc["text"], "metadata": doc.get("metadata", {}), "score": score})
        return sorted(results, key=lambda x: x["score"], reverse=True)





--------------------------------------------------------------------------------
File: memory/rerankers/temporal_federated.py
--------------------------------------------------------------------------------

class TemporalRetriever:
    """
    Retrieve documents based on temporal constraints (e.g., by timestamps).
    """
    def __init__(self, retriever, timestamp_field):
        """
        Args:
            retriever (object): Base retriever (e.g., dense, sparse).
            timestamp_field (str): Field containing document timestamps.
        """
        self.retriever = retriever
        self.timestamp_field = timestamp_field

    def query(self, query, start_time=None, end_time=None):
        """
        Retrieve documents within a temporal range.

        Args:
            query (str): Query string.
            start_time (str): Start time in ISO 8601 format.
            end_time (str): End time in ISO 8601 format.

        Returns:
            list: Retrieved documents matching the temporal range.
        """
        all_results = self.retriever.query(query)
        filtered_results = [
            doc for doc in all_results
            if self._is_within_range(doc[self.timestamp_field], start_time, end_time)
        ]
        return filtered_results

    def _is_within_range(self, timestamp, start_time, end_time):
        """Helper function to check if a timestamp is within a given range."""
        if start_time and timestamp < start_time:
            return False
        if end_time and timestamp > end_time:
            return False
        return True


class FederatedRetriever:
    """
    Retrieve documents from multiple retrievers (federated search).
    """
    def __init__(self, retrievers):
        """
        Args:
            retrievers (list): List of retrievers to federate.
        """
        self.retrievers = retrievers

    def query(self, query):
        """
        Federate queries across all retrievers.

        Args:
            query (str): Query string.

        Returns:
            list: Combined results from all retrievers.
        """
        results = []
        for retriever in self.retrievers:
            results.extend(retriever.query(query))
        return self._deduplicate_results(results)

    def _deduplicate_results(self, results):
        """Remove duplicates based on document IDs."""
        seen = set()
        deduplicated = []
        for result in results:
            doc_id = result.get("id")
            if doc_id not in seen:
                seen.add(doc_id)
                deduplicated.append(result)
        return deduplicated



--------------------------------------------------------------------------------
File: memory/rerankers/langchain.py
--------------------------------------------------------------------------------

class LangChainEmbeddingReranker(BaseReranker):
    def __init__(self, embedding_function):
        self.embedding_function = embedding_function

    def rerank(self, query, documents):
        """
        Rerank documents using LangChain embeddings.
        """
        query_embedding = self.embedding_function.embed_query(query)
        results = []
        for doc in documents:
            doc_embedding = self.embedding_function.embed_text(doc["text"])
            score = sum(q * d for q, d in zip(query_embedding, doc_embedding))
            results.append({"text": doc["text"], "metadata": doc.get("metadata", {}), "score": score})
        return sorted(results, key=lambda x: x["score"], reverse=True)





--------------------------------------------------------------------------------
File: memory/rerankers/workflows.py
--------------------------------------------------------------------------------

class CombinedRerankingWorkflow:
    """
    Combines multiple reranking strategies (e.g., semantic similarity and metadata-based scoring)
    into a unified reranking workflow.

    Attributes:
        rerankers (list): A list of reranker instances.
        weights (list): Corresponding weights for each reranker.
    """
    def __init__(self, rerankers, weights=None):
        """
        Initialize the workflow with rerankers and their weights.

        Args:
            rerankers (list): List of reranker instances (subclasses of BaseReranker).
            weights (list): List of weights for each reranker (default: equal weights).
        """
        self.rerankers = rerankers
        if weights is None:
            self.weights = [1.0 / len(rerankers)] * len(rerankers)
        else:
            self.weights = weights

    def run(self, query, documents):
        """
        Perform combined reranking.

        Args:
            query (str): The query string.
            documents (list): List of documents to rerank.

        Returns:
            list: Documents sorted by combined scores.
        """
        # Initialize scores for each document
        scores = {doc["text"]: 0.0 for doc in documents}

        # Iterate over each reranker and aggregate scores
        for reranker, weight in zip(self.rerankers, self.weights):
            ranked_docs = reranker.rerank(query, documents)
            for doc in ranked_docs:
                scores[doc["text"]] += doc["score"] * weight

        # Sort documents by combined scores
        combined_results = [{"text": doc, "score": score} for doc, score in scores.items()]
        return sorted(combined_results, key=lambda x: x["score"], reverse=True)


class MultiAgentRerankingWorkflow:
    """
    Multi-agent reranking strategy that utilizes multiple agents to score and aggregate
    reranking results for improved consensus-based reranking.

    Attributes:
        agents (list): A list of reranking agent instances.
        aggregation_function (callable): A function to combine scores (default: weighted average).
    """

    def __init__(self, agents, aggregation_function=None):
        """
        Initialize the multi-agent reranking workflow.

        Args:
            agents (list): List of reranking agent instances (subclasses of BaseReranker).
            aggregation_function (callable): Function to aggregate scores (default: weighted average).
        """
        self.agents = agents
        self.aggregation_function = aggregation_function or self.default_aggregation_function

    def default_aggregation_function(self, scores):
        """
        Default aggregation function (weighted average of scores).

        Args:
            scores (list): List of scores from all agents.

        Returns:
            float: Aggregated score.
        """
        total_weight = sum(weight for _, weight in scores)
        return sum(score * weight for score, weight in scores) / total_weight

    def run(self, query, documents):
        """
        Perform multi-agent reranking.

        Args:
            query (str): The query string.
            documents (list): List of documents to rerank.

        Returns:
            list: Documents sorted by aggregated scores.
        """
        # Collect scores from all agents
        scores = {doc["text"]: [] for doc in documents}

        for agent in self.agents:
            agent_ranking = agent.rerank(query, documents)
            for idx, doc in enumerate(agent_ranking):
                scores[doc["text"]].append((doc["score"], 1 / (idx + 1)))  # Weight based on rank

        # Aggregate scores
        aggregated_scores = {
            doc: self.aggregation_function(scores[doc])
            for doc in scores
        }

        # Sort documents by aggregated scores
        sorted_documents = sorted(
            [{"text": doc, "score": score} for doc, score in aggregated_scores.items()],
            key=lambda x: x["score"],
            reverse=True
        )
        return sorted_documents



--------------------------------------------------------------------------------
File: memory/rerankers/openai.py
--------------------------------------------------------------------------------

import json

class OpenAIReranker(BaseReranker):
    """
    A reranker using OpenAI's language models for relevance ranking.

    The reranker uses a structured JSON format for output, ensuring robustness and reliability.
    """

    def __init__(self, model_name="gpt-4"):
        """
        Initialize the OpenAIReranker.

        Args:
            model_name (str): Name of the OpenAI model to use (e.g., "gpt-4").
        """
        from langchain.llms import OpenAI
        self.llm = OpenAI(model_name=model_name)

    def rerank(self, query, documents):
        """
        Rerank documents based on relevance to the query.

        Args:
            query (str): The query string.
            documents (list): List of documents with 'text' and optional 'metadata'.

        Returns:
            list: Documents sorted by relevance score.
        """
        # Validate inputs
        if not isinstance(query, str):
            raise ValueError("Query must be a string.")
        if not isinstance(documents, list) or not all(isinstance(doc, dict) for doc in documents):
            raise ValueError("Documents must be a list of dictionaries.")

        # Construct prompt
        prompt = self._construct_prompt(query, documents)

        # Generate response
        try:
            response = self.llm(prompt)
            ranked_indices = self._parse_response(response, len(documents))
        except Exception as e:
            print(f"Error during reranking: {e}")
            return []

        # Map ranked indices back to documents
        ranked_documents = [documents[i] for i in ranked_indices]
        return ranked_documents

    def _construct_prompt(self, query, documents):
        """
        Construct a structured prompt for the OpenAI model.

        Args:
            query (str): The query string.
            documents (list): List of documents.

        Returns:
            str: The constructed prompt.
        """
        doc_list = "\n".join([f"{i + 1}. {doc['text']}" for i, doc in enumerate(documents)])
        prompt = (
            f"Rerank the following documents based on their relevance to the query.\n\n"
            f"Query: {query}\n\nDocuments:\n{doc_list}\n\n"
            f"Respond with a JSON array of indices in the order of relevance, e.g., [3, 1, 2]."
        )
        return prompt

    def _parse_response(self, response, num_docs):
        """
        Parse and validate the response from the OpenAI model.

        Args:
            response (str): The raw response from the model.
            num_docs (int): Number of documents in the input.

        Returns:
            list: List of ranked indices.

        Raises:
            ValueError: If the response is invalid or incomplete.
        """
        try:
            # Parse JSON response
            ranked_indices = json.loads(response)

            # Validate the output format
            if not isinstance(ranked_indices, list) or not all(
                isinstance(idx, int) and 1 <= idx <= num_docs for idx in ranked_indices
            ):
                raise ValueError("Response does not contain a valid list of indices.")
            if len(set(ranked_indices)) != num_docs:
                raise ValueError("Response indices are incomplete or contain duplicates.")

            # Convert to zero-based indexing
            return [idx - 1 for idx in ranked_indices]
        except (json.JSONDecodeError, ValueError) as e:
            raise ValueError(f"Invalid response format: {e}")




--------------------------------------------------------------------------------
File: memory/rerankers/misc.py
--------------------------------------------------------------------------------

class BM25Reranker(BaseReranker):
    def __init__(self, documents):
        from rank_bm25 import BM25Okapi
        self.bm25 = BM25Okapi([doc["text"].split() for doc in documents])

    def rerank(self, query, documents):
        """
        Rerank documents using BM25 scoring.
        """
        scores = self.bm25.get_scores(query.split())
        for doc, score in zip(documents, scores):
            doc["score"] = score
        return sorted(documents, key=lambda x: x["score"], reverse=True)


class MetadataReranker(BaseReranker):
    def __init__(self, metadata_field, reverse=True):
        self.metadata_field = metadata_field
        self.reverse = reverse

    def rerank(self, query, documents):
        """
        Rerank documents based on a metadata field.
        """
        return sorted(
            documents,
            key=lambda doc: doc.get("metadata", {}).get(self.metadata_field, 0),
            reverse=self.reverse
        )



--------------------------------------------------------------------------------
File: memory/adapters/database_adapter.py
--------------------------------------------------------------------------------

from abc import ABC, abstractmethod

class DatabaseAdapter(ABC):
    """
    Abstract base class for database adapters.

    Defines the interface that all database adapters must implement.
    """

    @abstractmethod
    def connect(self, config):
        """
        Establish a connection to the database.

        Args:
            config (dict): Configuration dictionary with required connection details.

        Returns:
            None

        Raises:
            ConnectionError: If the connection cannot be established.
        """
        pass

    @abstractmethod
    def insert(self, data):
        """
        Insert data into the database.

        Args:
            data (dict): The data to insert.

        Returns:
            bool: True if the operation was successful, False otherwise.

        Raises:
            ValueError: If data is invalid.
        """
        pass

    @abstractmethod
    def query(self, filters):
        """
        Query the database using the given filters.

        Args:
            filters (dict): A dictionary of query filters.

        Returns:
            list: A list of results matching the filters.

        Raises:
            ValueError: If filters are invalid.
        """
        pass

    @abstractmethod
    def delete(self, identifier):
        """
        Delete a record from the database.

        Args:
            identifier (str): The unique identifier of the record to delete.

        Returns:
            bool: True if the operation was successful, False otherwise.

        Raises:
            KeyError: If the identifier does not exist.
        """
        pass

    @abstractmethod
    def capabilities(self) -> Dict[str, bool]:
        raise NotImplementedError

# Example implementation of the DatabaseAdapter
class BaseExampleAdapter(DatabaseAdapter):
    """
    Example implementation of the DatabaseAdapter for demonstration purposes.

    This is a simple in-memory adapter that stores data in a Python dictionary.
    """

    def __init__(self):
        self._storage = {}

    def connect(self, config):
        """
        Simulate a connection to a database.

        Args:
            config (dict): Configuration dictionary (not used in this example).

        Returns:
            None
        """
        print("Connected to the in-memory database.")

    def insert(self, data):
        """
        Insert data into the in-memory database.

        Args:
            data (dict): The data to insert.

        Returns:
            bool: True if successful, False otherwise.
        """
        if 'id' not in data:
            raise ValueError("Data must contain an 'id' field.")
        self._storage[data['id']] = data
        return True

    def query(self, filters):
        """
        Query the in-memory database.

        Args:
            filters (dict): A dictionary of filters (e.g., {'field': 'value'}).

        Returns:
            list: A list of matching records.
        """
        results = []
        for record in self._storage.values():
            if all(record.get(k) == v for k, v in filters.items()):
                results.append(record)
        return results

    def delete(self, identifier):
        """
        Delete a record from the in-memory database.

        Args:
            identifier (str): The unique identifier of the record.

        Returns:
            bool: True if successful, False otherwise.
        """
        if identifier not in self._storage:
            raise KeyError(f"No record found with id '{identifier}'.")
        del self._storage[identifier]
        return True



--------------------------------------------------------------------------------
File: memory/adapters/langchain.py
--------------------------------------------------------------------------------

from .database_adapter import DatabaseAdapter

try:
    from langchain.embeddings.openai import OpenAIEmbeddings
except ImportError:
    OpenAIEmbeddings = None
    
try:
    from langchain.vectorstores import Pinecone
    import pinecone
except ImportError:
    Pinecone = None

try:
    from langchain.vectorstores import Weaviate
except ImportError:
    Weaviate = None

try:
    from langchain.vectorstores import Milvus
except ImportError:
    Milvus = None

try:
    from langchain.vectorstores import Qdrant
except ImportError:
    Qdrant = None

try:
    from langchain.vectorstores import SQLite
except ImportError:
    SQLite = None

try:
    from langchain.vectorstores import Redis
    import redis
except ImportError:
    Redis = None

try:
    from langchain.vectorstores import Chroma
    import chromadb
except ImportError:
    Chroma = None


class PineconeAdapter(DatabaseAdapter):
    def __init__(self, *args, **kwargs):
        if all(var is not None for var in (Pinecone, OpenAIEmbeddings)):
            pinecone.init(api_key=kwargs["api_key"], environment=kwargs["environment"])
            self.db = Pinecone(index_name=kwargs["index_name"], embedding_function=OpenAIEmbeddings())
        else:
            raise ValueError("Unsupported vector database. Make sure LangChain and Pinecone packages are installed.")

    def add_documents(self, documents):
        texts = [doc["text"] for doc in documents]
        metadata = [doc.get("metadata", {}) for doc in documents]
        self.db.add_texts(texts, metadatas=metadata)

    def add_documents_with_metadata(self, documents, metadata):
        self.db.add_texts(documents, metadatas=metadata)
        
    def query(self, query, filters=None):
        return self.db.similarity_search(query, filter=filters)

    def query_by_metadata(self, metadata_query, top_k=5):
        return self.db.similarity_search(query=None, filter=metadata_query, k=top_k)
        
    def delete(self, document_ids):
        for doc_id in document_ids:
            self.db.delete(doc_id)

    def delete_by_metadata(self, metadata_query):
        results = self.db.similarity_search(query=None, filter=metadata_query, k=1000)
        ids_to_delete = [doc["id"] for doc in results]
        for doc_id in ids_to_delete:
            self.db.delete(doc_id)
            
    def capabilities(self) -> Dict[str, bool]:
        return {
            "vector_search": True,  # Pinecone supports vector-based similarity search.
            "metadata_filtering": True,  # Metadata filtering is available.
            "semantic_search": True,  # Embedding-based semantic search supported via OpenAIEmbeddings.
        }


class WeaviateAdapter(DatabaseAdapter):
    """
    When working with LangChain's Weaviate integration, you can optionally provide 
    this client if you already have a preconfigured or specialized Weaviate client 
    setup. Otherwise, LangChain can initialize its own connection to the Weaviate 
    instance based on the url and authentication details provided.
    """
    def __init__(self, *args, **kwargs):
        if all(var is not None for var in (Weaviate, OpenAIEmbeddings)):
            self.db = Weaviate(
                url=kwargs["weaviate_url"],
                embedding_function=OpenAIEmbeddings(),
                client=kwargs.get("weaviate_client", None)  # Optional: Add Weaviate client instance if needed
            )
        else:
            raise ValueError("Unsupported vector database. Make sure LangChain and Weaviate packages are installed.")

    def add_documents(self, documents):
        texts = [doc["text"] for doc in documents]
        metadata = [doc.get("metadata", {}) for doc in documents]
        self.db.add_texts(texts, metadatas=metadata)

    def add_documents_with_metadata(self, documents, metadata):
        for doc, meta in zip(documents, metadata):
            self.db.add_text(doc, metadata=meta)

    def query(self, query, filters=None):
        return self.db.similarity_search(query, filter=filters)
        # return self.db.query(query, filters=filters) <-- Should we use the simple query instead?        

    def query_by_metadata(self, metadata_query, top_k=5):
        return self.db.query_by_metadata(metadata_query, top_k=top_k)

    def delete(self, document_ids):
        try:
            for doc_id in document_ids:
                self.db.delete_by_id(doc_id)
        except:
            # Not directly supported in LangChain's Weaviate implementation
            raise NotImplementedError("Document deletion is not yet supported in WeaviateAdapter.")

    def delete_by_metadata(self, metadata_query):
        self.db.delete_by_metadata(metadata_query)

    def capabilities(self) -> Dict[str, bool]:
        return {
            "vector_search": True,  # Weaviate supports vector-based similarity search.
            "metadata_filtering": True,  # Metadata filtering is available.
            "semantic_search": True,  # Semantic search is supported with embeddings.
        }


class MilvusAdapter(DatabaseAdapter):
    def __init__(self, *args, **kwargs):
        if all(var is not None for var in (Milvus, OpenAIEmbeddings)):
            self.db = Milvus(
                embedding_function=OpenAIEmbeddings(),
                collection_name=kwargs["collection_name"],
                connection_args={
                    "host": kwargs["milvus_host"],
                    "port": kwargs["milvus_port"]
                }
            )
        else:
            raise ValueError("Unsupported vector database. Make sure LangChain and Milvus packages are installed.")

    def add_documents(self, documents):
        texts = [doc["text"] for doc in documents]
        metadata = [doc.get("metadata", {}) for doc in documents]
        self.db.add_texts(texts, metadatas=metadata)

    def add_documents_with_metadata(self, documents, metadata):
        self.db.add_texts(documents, metadatas=metadata)

    def query(self, query, filters=None):
        return self.db.similarity_search(query, filter=filters)

    def query_by_metadata(self, metadata_query, top_k=5):
        return self.db.query_by_metadata(metadata_query, top_k=top_k)

    def delete(self, document_ids):
        try:
            for doc_id in document_ids:
                self.db.delete_by_id(doc_id)
        except:
            # Not directly supported in LangChain's Milvus implementation
            raise NotImplementedError("Document deletion is not yet supported in MilvusAdapter.")

    def delete_by_metadata(self, metadata_query):
        self.db.delete_by_metadata(metadata_query)

    def capabilities(self) -> Dict[str, bool]:
        return {
            "vector_search": True,  # Milvus supports vector-based similarity search.
            "metadata_filtering": True,  # Metadata filtering is available.
            "semantic_search": True,  # Semantic search is supported with embeddings.
        }


class QdrantAdapter(DatabaseAdapter):
    def __init__(self, *args, **kwargs):
        if all(var is not None for var in (Qdrant, OpenAIEmbeddings)):
            self.db = Qdrant(
                host=kwargs["qdrant_host"],
                port=kwargs["qdrant_port"],
                embedding_function=OpenAIEmbeddings(),
                collection_name=kwargs["collection_name"]
            )
        else:
            raise ValueError("Unsupported vector database. Make sure LangChain and Qdrant packages are installed.")

    def add_documents(self, documents):
        texts = [doc["text"] for doc in documents]
        metadata = [doc.get("metadata", {}) for doc in documents]
        self.db.add_texts(texts, metadatas=metadata)

    def query(self, query, filters=None):
        return self.db.similarity_search(query, filter=filters)

    def delete(self, document_ids):
        self.db.delete(ids=document_ids)

    def capabilities(self) -> Dict[str, bool]:
        return {
            "vector_search": True,  # Qdrant supports vector-based similarity search.
            "metadata_filtering": True,  # Metadata filtering is available.
            "semantic_search": True,  # Embedding-based semantic search supported.
        }


class SQLiteAdapter(DatabaseAdapter):
    def __init__(self, *args, **kwargs):
        if all(var is not None for var in (SQLite, OpenAIEmbeddings)):
            self.db = SQLite(
                embedding_function=OpenAIEmbeddings(),
                database_path=kwargs["database_path"],
                table_name=kwargs["table_name"]
            )
        else:
            raise ValueError("Unsupported database. Make sure LangChain and SQLite packages are installed.")

    def add_documents(self, documents):
        texts = [doc["text"] for doc in documents]
        metadata = [doc.get("metadata", {}) for doc in documents]
        self.db.add_texts(texts, metadatas=metadata)

    def query(self, query, filters=None):
        return self.db.similarity_search(query, filter=filters)

    def delete(self, document_ids):
        self.db.delete(ids=document_ids)

    def capabilities(self) -> Dict[str, bool]:
        return {
            "vector_search": True,  # LangChain's SQLite integration supports vector-based search.
            "metadata_filtering": True,  # Metadata filtering is implemented via SQL queries.
            "semantic_search": True,  # Embedding-based semantic search supported.
        }


class RedisAdapter(DatabaseAdapter):
    def __init__(self, *args, **kwargs):
        if Redis:
            self.db = Redis(index_name=kwargs["index_name"], redis_url=kwargs["redis_url"])
        else:
            raise ValueError("Redis package is not installed.")

    def add_documents(self, documents):
        texts = [doc["text"] for doc in documents]
        metadata = [doc.get("metadata", {}) for doc in documents]
        self.db.add_texts(texts, metadatas=metadata)

    def add_documents_with_metadata(self, documents, metadata):
        self.db.add_texts(documents, metadatas=metadata)

    def query(self, query, filters=None):
        return self.db.similarity_search(query, filter=filters)

    def query_by_metadata(self, metadata_query, top_k=5):
        return self.db.similarity_search(query=None, filter=metadata_query, k=top_k)

    def delete(self, document_ids):
        for doc_id in document_ids:
            self.db.delete(doc_id)

    def delete_by_metadata(self, metadata_query):
        self.db.delete(filter=metadata_query)


class ChromaAdapter(DatabaseAdapter):
    def __init__(self, *args, **kwargs):
        if Chroma:
            self.db = Chroma(
                collection_name=kwargs["collection_name"],
                embedding_function=kwargs["embedding_function"],
            )
        else:
            raise ValueError("Chroma package is not installed.")

    def add_documents(self, documents):
        texts = [doc["text"] for doc in documents]
        metadata = [doc.get("metadata", {}) for doc in documents]
        self.db.add_texts(texts, metadatas=metadata)

    def add_documents_with_metadata(self, documents, metadata):
        self.db.add_texts(documents, metadatas=metadata)

    def query(self, query, filters=None):
        return self.db.similarity_search(query, filter=filters)

    def query_by_metadata(self, metadata_query, top_k=5):
        return self.db.similarity_search(query=None, filter=metadata_query, k=top_k)

    def delete(self, document_ids):
        for doc_id in document_ids:
            self.db.delete(doc_id)

    def delete_by_metadata(self, metadata_query):
        self.db.delete(filter=metadata_query)



--------------------------------------------------------------------------------
File: memory/adapters/workflows.py
--------------------------------------------------------------------------------

class HybridRetrievalWorkflow:
    """
    Combines dense and sparse retrieval mechanisms to balance semantic relevance
    and keyword-based matching (e.g., using embeddings and BM25).

    Attributes:
        dense_retriever (object): A dense retriever (e.g., Pinecone or FAISS).
        sparse_retriever (object): A sparse retriever (e.g., BM25).

    Usage Example:
    --------------
    # Initialize retrievers
    dense_retriever = PineconeAdapter(pinecone_instance)
    sparse_retriever = BM25Retriever(documents)

    # Create hybrid workflow
    hybrid_workflow = HybridRetrievalWorkflow(dense_retriever, sparse_retriever)

    # Perform retrieval
    results = hybrid_workflow.run("What is LangSwarm?")
    print("Hybrid Retrieval Results:", results)
    """
    def __init__(self, dense_retriever, sparse_retriever):
        self.dense_retriever = dense_retriever
        self.sparse_retriever = sparse_retriever

    def run(self, query):
        """
        Perform hybrid retrieval.

        Args:
            query (str): The user's query.

        Returns:
            list: Merged and deduplicated results.
        """
        dense_results = self.dense_retriever.query(query)
        sparse_results = self.sparse_retriever.query(query)

        # Merge and deduplicate results
        combined = {doc['text']: doc for doc in dense_results + sparse_results}
        return list(combined.values())


class MultiSourceRetrievalWorkflow:
    """
    Retrieves data from multiple sources and aggregates results.

    Attributes:
        retrievers (list): A list of retrievers for different sources.

    Usage Example:
    --------------
    # Initialize multiple retrievers
    retriever_1 = PineconeAdapter(pinecone_instance)
    retriever_2 = SQLRetriever(database_uri)

    # Create multi-source workflow
    multi_source_workflow = MultiSourceRetrievalWorkflow([retriever_1, retriever_2])

    # Perform retrieval
    results = multi_source_workflow.run("What is LangSwarm?")
    print("Multi-Source Retrieval Results:", results)
    """
    def __init__(self, retrievers):
        self.retrievers = retrievers

    def run(self, query):
        """
        Perform retrieval from all sources.

        Args:
            query (str): The user's query.

        Returns:
            list: Combined results from all sources.
        """
        all_results = []
        for retriever in self.retrievers:
            all_results.extend(retriever.query(query))
        return all_results


class TemporalRetrievalWorkflow:
    """
    Retrieves documents based on temporal constraints.

    Attributes:
        retriever (object): The base retriever.
        time_filter (function): A function to filter results by time.

    Usage Example:
    --------------
    # Define a time filter function
    def recent_filter(doc):
        return doc.get("metadata", {}).get("timestamp") >= "2025-01-01"

    # Initialize temporal workflow
    retriever = PineconeAdapter(pinecone_instance)
    temporal_workflow = TemporalRetrievalWorkflow(retriever, recent_filter)

    # Perform retrieval
    results = temporal_workflow.run("What is LangSwarm?")
    print("Temporal Retrieval Results:", results)
    """
    def __init__(self, retriever, time_filter):
        self.retriever = retriever
        self.time_filter = time_filter

    def run(self, query):
        """
        Retrieve and filter results by time.

        Args:
            query (str): The user's query.

        Returns:
            list: Filtered results.
        """
        results = self.retriever.query(query)
        return [doc for doc in results if self.time_filter(doc)]


class FederatedRetrievalWorkflow:
    """
    Retrieves data from distributed databases or indices.

    Attributes:
        retrievers (list): A list of distributed retrievers.

    Usage Example:
    --------------
    # Initialize distributed retrievers
    retriever_1 = PineconeAdapter(pinecone_instance)
    retriever_2 = WeaviateAdapter(weaviate_url)

    # Create federated workflow
    federated_workflow = FederatedRetrievalWorkflow([retriever_1, retriever_2])

    # Perform retrieval
    results = federated_workflow.run("What is LangSwarm?")
    print("Federated Retrieval Results:", results)
    """
    def __init__(self, retrievers):
        self.retrievers = retrievers

    def run(self, query):
        """
        Perform federated retrieval.

        Args:
            query (str): The user's query.

        Returns:
            list: Combined results from all sources.
        """
        all_results = []
        for retriever in self.retrievers:
            all_results.extend(retriever.query(query))
        return all_results


class CrossDomainRetrievalWorkflow:
    """
    Routes queries to domain-specific retrievers.

    Attributes:
        domain_retrievers (dict): A dictionary mapping domains to retrievers.

    Usage Example:
    --------------
    # Define retrievers for each domain
    retriever_medical = PineconeAdapter(pinecone_instance)
    retriever_legal = FAISSAdapter(index_path="legal_index.json")

    # Map retrievers to domains
    domain_retrievers = {
        "medical": retriever_medical,
        "legal": retriever_legal
    }

    # Create cross-domain workflow
    cross_domain_workflow = CrossDomainRetrievalWorkflow(domain_retrievers)

    # Perform domain-specific retrieval
    results = cross_domain_workflow.run("What is LangSwarm?", "medical")
    print("Cross-Domain Retrieval Results:", results)
    """
    def __init__(self, domain_retrievers):
        self.domain_retrievers = domain_retrievers

    def run(self, query, domain):
        """
        Retrieve data from the appropriate domain.

        Args:
            query (str): The user's query.
            domain (str): The target domain.

        Returns:
            list: Results from the specified domain.
        """
        retriever = self.domain_retrievers.get(domain)
        if retriever is None:
            raise ValueError(f"No retriever found for domain: {domain}")
        return retriever.query(query)



--------------------------------------------------------------------------------
File: memory/adapters/langswarm.py
--------------------------------------------------------------------------------

from typing import Dict  # For typing annotations
from langswarm.memory.adapters.database_adapter import DatabaseAdapter

try:
    import sqlite3
except ImportError:
    sqlite3 = None

try:
    import redis
except ImportError:
    redis = None

try:
    from chromadb import Client as ChromaDB
    from chromadb.config import Settings
except ImportError:
    ChromaDB = None

try:
    from google.cloud import storage
except ImportError:
    storage = None

try:
    from elasticsearch import Elasticsearch
except ImportError:
    storage = None


class SQLiteAdapter(DatabaseAdapter):
    def __init__(self, db_path="memory.db"):
        if any(var is None for var in (sqlite3)):
            raise ValueError("Unsupported database. Make sure sqlite3 is installed.")
            
        self.db_path = db_path
        self._initialize_db()

    def _initialize_db(self):
        with self._get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS memory (
                    key TEXT PRIMARY KEY,
                    value TEXT NOT NULL,
                    metadata TEXT
                )
                """
            )
            conn.commit()

    def _get_connection(self):
        return sqlite3.connect(self.db_path)

    def add_documents(self, documents):
        with self._get_connection() as conn:
            cursor = conn.cursor()
            for doc in documents:
                key = doc.get("key", "")
                value = doc.get("text", "")
                metadata = str(doc.get("metadata", {}))
                cursor.execute(
                    "INSERT OR REPLACE INTO memory (key, value, metadata) VALUES (?, ?, ?)",
                    (key, value, metadata),
                )
            conn.commit()

    def query(self, query, filters=None):
        with self._get_connection() as conn:
            cursor = conn.cursor()
            sql_query = "SELECT key, value, metadata FROM memory WHERE value LIKE ?"
            params = [f"%{query}%"]

            if filters:
                for field, value in filters.items():
                    sql_query += f" AND metadata LIKE ?"
                    params.append(f'%"{field}": "{value}"%')

            cursor.execute(sql_query, params)
            rows = cursor.fetchall()
            return [
                {"key": row[0], "text": row[1], "metadata": eval(row[2])} for row in rows
            ]

    def delete(self, document_ids):
        with self._get_connection() as conn:
            cursor = conn.cursor()
            for doc_id in document_ids:
                cursor.execute("DELETE FROM memory WHERE key = ?", (doc_id,))
            conn.commit()

    def capabilities(self) -> Dict[str, bool]:
        return {
            "vector_search": False,  # SQLite does not support vector-based search.
            "metadata_filtering": True,  # Supports metadata filtering through SQL queries.
            "semantic_search": False,  # Requires external embeddings for semantic capabilities.
        }


class RedisAdapter(DatabaseAdapter):
    def __init__(self, redis_url="redis://localhost:6379/0"):
        if any(var is None for var in (redis)):
            raise ValueError("Unsupported database. Make sure sqlite3 is installed.")
            
        self.client = redis.StrictRedis.from_url(redis_url)

    def add_documents(self, documents):
        for doc in documents:
            key = doc.get("key", "")
            value = doc.get("text", "")
            metadata = doc.get("metadata", {})
            self.client.set(key, str({"value": value, "metadata": metadata}))

    def query(self, query, filters=None):
        keys = self.client.keys("*")
        results = []
        for key in keys:
            entry = eval(self.client.get(key).decode())
            if query.lower() in entry["value"].lower():
                if filters and not all(
                    entry["metadata"].get(k) == v for k, v in filters.items()
                ):
                    continue
                results.append({"key": key.decode(), **entry})
        return results

    def delete(self, document_ids):
        for doc_id in document_ids:
            self.client.delete(doc_id)

    def capabilities(self) -> Dict[str, bool]:
        return {
            "vector_search": False,  # Redis requires vector extensions like RediSearch for this.
            "metadata_filtering": True,  # Supports metadata-based filtering if implemented.
            "semantic_search": False,  # No built-in semantic search support.
        }


class ChromaDBAdapter(DatabaseAdapter):
    def __init__(self, collection_name="shared_memory", persist_directory=None):
        if any(var is None for var in (ChromaDB)):
            raise ValueError("Unsupported database. Make sure ChromaDB is installed.")
            
        self.client = ChromaDB(Settings(persist_directory=persist_directory))
        self.collection = self.client.get_or_create_collection(name=collection_name)

    def add_documents(self, documents):
        for doc in documents:
            key = doc.get("key", "")
            value = doc.get("text", "")
            metadata = doc.get("metadata", {})
            self.collection.add(ids=[key], documents=[value], metadatas=[metadata])

    def query(self, query, filters=None):
        results = self.collection.query(query_text=query)
        if filters:
            return [
                {
                    "key": res["id"],
                    "text": res["document"],
                    "metadata": res["metadata"],
                }
                for res in results
                if all(
                    res["metadata"].get(k) == v for k, v in filters.items()
                )
            ]
        return results

    def delete(self, document_ids):
        for doc_id in document_ids:
            self.collection.delete(ids=[doc_id])

    def capabilities(self) -> Dict[str, bool]:
        return {
            "vector_search": True,  # Chroma supports vector-based search.
            "metadata_filtering": True,  # Metadata filtering is a core feature.
            "semantic_search": True,  # Supports embeddings for semantic search.
        }


class GCSAdapter(DatabaseAdapter):
    def __init__(self, bucket_name, prefix="shared_memory/"):
        if any(var is None for var in (storage)):
            raise ValueError("Unsupported database. Make sure google cloud storage is installed.")
            
        self.client = storage.Client()
        self.bucket = self.client.bucket(bucket_name)
        self.prefix = prefix

    def add_documents(self, documents):
        for doc in documents:
            key = f"{self.prefix}{doc.get('key', '')}"
            value = doc.get("text", "")
            metadata = doc.get("metadata", {})
            blob = self.bucket.blob(key)
            blob.upload_from_string(str({"value": value, "metadata": metadata}))

    def query(self, query, filters=None):
        blobs = list(self.client.list_blobs(self.bucket, prefix=self.prefix))
        results = []
        for blob in blobs:
            entry = eval(blob.download_as_text())
            if query.lower() in entry["value"].lower():
                if filters and not all(
                    entry["metadata"].get(k) == v for k, v in filters.items()
                ):
                    continue
                results.append({"key": blob.name[len(self.prefix):], **entry})
        return results

    def delete(self, document_ids):
        for doc_id in document_ids:
            blob = self.bucket.blob(f"{self.prefix}{doc_id}")
            if blob.exists():
                blob.delete()

    def capabilities(self) -> Dict[str, bool]:
        return {
            "vector_search": False,  # GCS is a storage solution, not a vector database.
            "metadata_filtering": True,  # Metadata filtering implemented via stored metadata.
            "semantic_search": False,  # Semantic capabilities not supported natively.
        }


class ElasticsearchAdapter(DatabaseAdapter):
    def __init__(self, *args, **kwargs):
        if Elasticsearch:
            self.db = Elasticsearch(kwargs["connection_string"])
        else:
            raise ValueError("Elasticsearch package is not installed.")

    def add_documents(self, documents):
        for doc in documents:
            self.db.index(index="documents", body={"text": doc["text"], "metadata": doc.get("metadata", {})})

    def add_documents_with_metadata(self, documents, metadata):
        for doc, meta in zip(documents, metadata):
            self.db.index(index="documents", body={"text": doc, "metadata": meta})

    def query(self, query, filters=None):
        body = {"query": {"match": {"text": query}}}
        if filters:
            body["query"] = {"bool": {"must": [{"match": {"text": query}}], "filter": [{"term": filters}]}}
        return self.db.search(index="documents", body=body)

    def query_by_metadata(self, metadata_query, top_k=5):
        body = {"query": {"bool": {"filter": [{"term": metadata_query}]}}}
        return self.db.search(index="documents", body=body, size=top_k)

    def delete(self, document_ids):
        for doc_id in document_ids:
            self.db.delete(index="documents", id=doc_id)

    def delete_by_metadata(self, metadata_query):
        body = {"query": {"bool": {"filter": [{"term": metadata_query}]}}}
        self.db.delete_by_query(index="documents", body=body)

    def capabilities(self) -> Dict[str, bool]:
        return {
            "vector_search": True,  # Elasticsearch supports vector search with extensions like dense_vector.
            "metadata_filtering": True,  # Strong metadata filtering capabilities.
            "semantic_search": True,  # Can be configured for semantic search using embeddings.
        }



--------------------------------------------------------------------------------
File: memory/adapters/llamaindex.py
--------------------------------------------------------------------------------

from .database_adapter import DatabaseAdapter

try:
    from llama_index import Document
except ImportError:
    Document = None
    
try:
    from llama_index import GPTSimpleVectorIndex
except ImportError:
    GPTSimpleVectorIndex = None

try:
    import pinecone
    from llama_index import PineconeIndex
except ImportError:
    pinecone = None
    PineconeIndex = None

try:
    from llama_index import WeaviateIndex
except ImportError:
    WeaviateIndex = None

try:
    from llama_index import FAISSIndex
except ImportError:
    FAISSIndex = None

try:
    from llama_index import SQLDatabase, SQLIndex
except ImportError:
    SQLDatabase = None
    SQLIndex = None


class LlamaIndexDiskAdapter(DatabaseAdapter):
    def __init__(self, index_path="index.json"):
        if all(var is not None for var in (GPTSimpleVectorIndex, Document)):
            try:
                self.index = GPTSimpleVectorIndex.load_from_disk(index_path)
            except FileNotFoundError:
                self.index = GPTSimpleVectorIndex([])
        else:
            raise ValueError("Unsupported database. Make sure LlamaIndex is installed.")

    def add_documents(self, documents):
        docs = [Document(text=doc["text"], metadata=doc.get("metadata", {})) for doc in documents]
        self.index.insert(docs)
        self.index.save_to_disk()

    def query(self, query, filters=None):
        results = self.index.query(query)
        if filters:
            results = [res for res in results if all(res.extra_info.get(k) == v for k, v in filters.items())]
        return results

    def delete(self, document_ids):
        raise NotImplementedError("Delete functionality not implemented for LlamaIndex")

    def capabilities(self) -> Dict[str, bool]:
        return {
            "vector_search": True,
            "metadata_filtering": True,
            "semantic_search": True,
        }


class LlamaIndexPineconeAdapter(LlamaIndexAdapter):
    """
    Adapter for Pinecone integration with LlamaIndex.

    Setup:
        1. Install Pinecone: `pip install pinecone-client`.
        2. Initialize Pinecone with your API key and environment:
           ```
           pinecone.init(api_key="your-api-key", environment="your-environment")
           ```

    Usage:
        Add, query, and manage documents in a Pinecone-backed vector index.
    """
    def __init__(self, index_name="pinecone-index"):
        if pinecone is None or PineconeIndex is None:
            raise ImportError("Pinecone or LlamaIndex is not installed. Please install the required packages.")

        self.index_name = index_name
        if index_name not in pinecone.list_indexes():
            pinecone.create_index(index_name, dimension=768)  # Update dimension based on your embedding model
        self.index = PineconeIndex(index_name=index_name)

    def add_documents(self, documents):
        docs = [Document(text=doc["text"], metadata=doc.get("metadata", {})) for doc in documents]
        self.index.insert(docs)

    def query(self, query_text):
        return self.index.query(query_text)

    def delete(self, document_ids):
        self.index.delete(document_ids)

    def capabilities(self) -> Dict[str, bool]:
        return {
            "vector_search": True,
            "metadata_filtering": True,
            "semantic_search": True,
        }


class LlamaIndexWeaviateAdapter(LlamaIndexAdapter):
    """
    Adapter for Weaviate integration with LlamaIndex.

    Setup:
        1. Install Weaviate client: `pip install weaviate-client`.
        2. Ensure you have a running Weaviate instance and its URL.

    Usage:
        Add, query, and manage documents in a Weaviate-backed vector index.
    """
    def __init__(self, weaviate_url):
        if WeaviateIndex is None:
            raise ImportError("Weaviate or LlamaIndex is not installed. Please install the required packages.")

        self.index = WeaviateIndex(weaviate_url=weaviate_url)

    def add_documents(self, documents):
        docs = [Document(text=doc["text"], metadata=doc.get("metadata", {})) for doc in documents]
        self.index.insert(docs)

    def query(self, query_text):
        return self.index.query(query_text)

    def delete(self, document_ids):
        raise NotImplementedError("Document deletion is not yet supported for Weaviate.")

    def capabilities(self) -> Dict[str, bool]:
        return {
            "vector_search": True,
            "metadata_filtering": True,
            "semantic_search": True,
        }


class LlamaIndexFAISSAdapter(LlamaIndexAdapter):
    """
    Adapter for FAISS integration with LlamaIndex.

    Setup:
        1. Install FAISS: `pip install faiss-cpu`.
        2. Initialize a FAISS index for local vector storage.

    Usage:
        Add, query, and manage documents in a FAISS-backed vector index.
    """
    def __init__(self, index_path="faiss_index.json"):
        if FAISSIndex is None:
            raise ImportError("FAISS or LlamaIndex is not installed. Please install the required packages.")

        try:
            self.index = FAISSIndex.load_from_disk(index_path)
        except FileNotFoundError:
            self.index = FAISSIndex([])

    def add_documents(self, documents):
        docs = [Document(text=doc["text"], metadata=doc.get("metadata", {})) for doc in documents]
        self.index.insert(docs)
        self.index.save_to_disk("faiss_index.json")

    def query(self, query_text):
        return self.index.query(query_text)

    def delete(self, document_ids):
        raise NotImplementedError("Document deletion is not yet supported for FAISS.")

    def capabilities(self) -> Dict[str, bool]:
        return {
            "vector_search": True,
            "metadata_filtering": False,  # FAISS lacks native metadata filtering.
            "semantic_search": True,
        }


class LlamaIndexSQLAdapter(LlamaIndexAdapter):
    """
    Adapter for SQL integration with LlamaIndex.

    Setup:
        1. Install a SQL database driver (e.g., `pip install sqlite`).
        2. Create and configure your database URI.

    Usage:
        Add, query, and manage documents in a SQL-backed index.
    """
    def __init__(self, database_uri, index_path="sql_index.json"):
        if SQLDatabase is None or SQLIndex is None:
            raise ImportError("SQLDatabase or LlamaIndex is not installed. Please install the required packages.")

        self.sql_db = SQLDatabase(database_uri=database_uri)
        try:
            self.index = SQLIndex.load_from_disk(index_path)
        except FileNotFoundError:
            self.index = SQLIndex([], sql_database=self.sql_db)

    def add_documents(self, documents):
        for doc in documents:
            self.sql_db.insert({"text": doc["text"], **doc.get("metadata", {})})
        self.index.refresh()

    def query(self, query_text):
        return self.index.query(query_text)

    def delete(self, document_ids):
        raise NotImplementedError("Document deletion is not yet supported for SQL.")

    def capabilities(self) -> Dict[str, bool]:
        return {
            "vector_search": False,
            "metadata_filtering": True,
            "semantic_search": False,
        }



--------------------------------------------------------------------------------
File: memory/templates/biomed.py
--------------------------------------------------------------------------------

from langswarm.agent import LangSwarmAgent
from retrievers import BiomedicalRetriever  # Placeholder for your retriever implementation
from rerankers import BiomedicalReranker  # Placeholder for your reranker implementation

class BiomedicalSearchWorkflow:
    """
    Workflow for retrieving and ranking biomedical literature for research or clinical use.

    Specification:
    - Initialize a LangSwarm agent.
    - Set up a biomedical retriever and a domain-specific reranker.
    - Fetch, process, and load biomedical data into the retriever backend.
    - Handle user queries and provide ranked biomedical document results.
    - Implement fallback for cases where no documents are retrieved or reranked.
    """

    def __init__(self, retriever_config, reranker_config, biomedical_data):
        """
        Initialize the biomedical data search workflow.

        Args:
            retriever_config (dict): Configuration for setting up the biomedical retriever.
            reranker_config (dict): Configuration for the biomedical-specific reranker.
            biomedical_data (list): Data for populating the retriever backend.
        """
        # Initialize LangSwarm agent
        self.agent = LangSwarmAgent()

        # Initialize retriever and reranker
        self.retriever = BiomedicalRetriever(**retriever_config)
        self.reranker = BiomedicalReranker(**reranker_config)

        # Data Fetch and Processing
        self.processed_data = self.process_data(biomedical_data)

        # Data Load
        self.load_data_to_retriever()

    def process_data(self, raw_data):
        """
        Process the raw data into retriever-compatible formats.

        Args:
            raw_data (list): List of raw biomedical data.

        Returns:
            list: Processed data ready for loading.
        """
        return [{"text": entry.strip(), "metadata": {"domain": "biomedical"}} for entry in raw_data]

    def load_data_to_retriever(self):
        """
        Load the processed data into the retriever backend.
        """
        self.retriever.add_documents(self.processed_data)

    def run(self, query):
        """
        Execute the biomedical data search workflow.

        Args:
            query (str): User's query.

        Returns:
            str: Response generated by the LangSwarm agent, or None if no results are found.
        """
        # Retrieve documents
        documents = self.retriever.query(query)

        if not documents:
            print("No biomedical documents retrieved.")
            return None

        # Rerank documents
        reranked_documents = self.reranker.rerank(query, documents)

        if not reranked_documents:
            print("No documents could be reranked.")
            return None

        # Concatenate input for the agent
        context = reranked_documents[0]["text"]
        agent_input = f"Query: {query}\nContext: {context}"

        # Generate and return response
        return self.agent.generate_response(agent_input)


# **Usage Example**

if __name__ == "__main__":
    # Example configuration and biomedical data
    retriever_config = {"backend": "pinecone", "api_key": "your-api-key"}
    reranker_config = {"model_name": "biobert"}  # Example biomedical-specific model
    biomedical_data = [
        "Study 1: Impact of LangSwarm on clinical decision-making.",
        "Study 2: Reinforcement learning in personalized medicine.",
        "Study 3: Large language models in drug discovery."
    ]

    # Initialize workflow
    biomedical_search = BiomedicalSearchWorkflow(retriever_config, reranker_config, biomedical_data)

    # User query
    user_query = "How are large language models used in drug discovery?"

    # Run the workflow
    response = biomedical_search.run(user_query)

    if response:
        print("Biomedical Search Response:", response)
    else:
        print("No relevant response could be generated.")



--------------------------------------------------------------------------------
File: memory/templates/temporal_federated.py
--------------------------------------------------------------------------------

from memory.retrievers.temporal_federated import TemporalRetriever, FederatedRetriever

class TemporalFederatedWorkflow:
    """
    Workflow combining temporal and federated retrieval.
    """

    def __init__(self, dense_config, sparse_config, timestamp_field, retriever_configs):
        """
        Initialize the workflow.

        Args:
            dense_config (dict): Configuration for dense retriever.
            sparse_config (dict): Configuration for sparse retriever.
            timestamp_field (str): Field with timestamps.
            retriever_configs (list): List of federated retriever configs.
        """
        # Initialize temporal retrievers
        self.dense_retriever = TemporalRetriever(**dense_config, timestamp_field=timestamp_field)
        self.sparse_retriever = TemporalRetriever(**sparse_config, timestamp_field=timestamp_field)

        # Initialize federated retriever
        retrievers = [config["retriever"](**config["params"]) for config in retriever_configs]
        self.federated_retriever = FederatedRetriever(retrievers)

    def run(self, query, start_time=None, end_time=None):
        """
        Execute temporal and federated retrieval.

        Args:
            query (str): User query.
            start_time (str): Start time.
            end_time (str): End time.

        Returns:
            list: Retrieved documents.
        """
        # Perform temporal retrieval
        dense_results = self.dense_retriever.query(query, start_time, end_time)
        sparse_results = self.sparse_retriever.query(query, start_time, end_time)

        # Combine temporal results
        temporal_results = dense_results + sparse_results

        # Perform federated retrieval
        federated_results = self.federated_retriever.query(query)

        # Merge all results
        return temporal_results + federated_results



--------------------------------------------------------------------------------
File: memory/templates/customer_support.py
--------------------------------------------------------------------------------

from langswarm.agent import LangSwarmAgent
from retrievers import FAQRetriever, TicketRetriever  # Placeholder for your retriever implementations
from rerankers import CombinedReranker  # Placeholder for your reranker implementation

class CustomerSupportWorkflow:
    """
    Workflow for automating customer support by retrieving and ranking relevant FAQs and ticket data.

    Specification:
    - Initialize a LangSwarm agent.
    - Set up retrievers for FAQs and ticket data, along with a combined reranker.
    - Fetch, process, and load data into respective retriever backends.
    - Handle user queries and generate contextual responses.
    - Implement fallback for cases where no documents are retrieved or reranked.
    """

    def __init__(self, faq_retriever_config, ticket_retriever_config, reranker_config, faq_data, ticket_data):
        """
        Initialize the customer support workflow.

        Args:
            faq_retriever_config (dict): Configuration for setting up the FAQ retriever.
            ticket_retriever_config (dict): Configuration for setting up the ticket retriever.
            reranker_config (dict): Configuration for the combined reranker.
            faq_data (list): Data for populating the FAQ retriever.
            ticket_data (list): Data for populating the ticket retriever.
        """
        # Initialize LangSwarm agent
        self.agent = LangSwarmAgent()

        # Initialize retrievers
        self.faq_retriever = FAQRetriever(**faq_retriever_config)
        self.ticket_retriever = TicketRetriever(**ticket_retriever_config)

        # Initialize reranker
        self.reranker = CombinedReranker(**reranker_config)

        # Data Fetch and Processing
        self.processed_faq_data = self.process_data(faq_data)
        self.processed_ticket_data = self.process_data(ticket_data)

        # Data Load
        self.load_data_to_retrievers()

    def process_data(self, raw_data):
        """
        Process the raw data into retriever-compatible formats.

        Args:
            raw_data (list): List of raw data.

        Returns:
            list: Processed data ready for loading.
        """
        return [{"text": entry.strip(), "metadata": {}} for entry in raw_data]

    def load_data_to_retrievers(self):
        """
        Load the processed data into respective retriever backends.
        """
        self.faq_retriever.add_documents(self.processed_faq_data)
        self.ticket_retriever.add_documents(self.processed_ticket_data)

    def run(self, query):
        """
        Execute the customer support workflow.

        Args:
            query (str): User's query.

        Returns:
            str: Response generated by the LangSwarm agent, or None if no results are found.
        """
        # Retrieve FAQs and ticket data
        faq_documents = self.faq_retriever.query(query)
        ticket_documents = self.ticket_retriever.query(query)

        if not faq_documents and not ticket_documents:
            print("No documents retrieved from FAQs or tickets.")
            return None

        # Combine documents and rerank
        combined_documents = faq_documents + ticket_documents
        reranked_documents = self.reranker.rerank(query, combined_documents)

        if not reranked_documents:
            print("No documents could be reranked.")
            return None

        # Concatenate input for the agent
        context = reranked_documents[0]["text"]
        agent_input = f"Query: {query}\nContext: {context}"

        # Generate and return response
        return self.agent.generate_response(agent_input)


# **Usage Example**

if __name__ == "__main__":
    # Example configurations and data
    faq_retriever_config = {"backend": "pinecone", "api_key": "your-api-key"}
    ticket_retriever_config = {"backend": "sqlite", "connection_string": "sqlite:///tickets.db"}
    reranker_config = {"weights": [0.6, 0.4]}  # Example weights for FAQs and tickets
    faq_data = ["What are the support hours?", "How can I reset my password?"]
    ticket_data = ["Ticket 123: Reset password issue.", "Ticket 456: Unable to access account."]

    # Initialize workflow
    customer_support = CustomerSupportWorkflow(
        faq_retriever_config, ticket_retriever_config, reranker_config, faq_data, ticket_data
    )

    # User query
    user_query = "How do I reset my password?"

    # Run the workflow
    response = customer_support.run(user_query)

    if response:
        print("Customer Support Response:", response)
    else:
        print("No relevant response could be generated.")



--------------------------------------------------------------------------------
File: memory/templates/legal.py
--------------------------------------------------------------------------------

from langswarm.agent import LangSwarmAgent
from retrievers import LegalRetriever  # Placeholder for your retriever implementation
from rerankers import LegalReranker  # Placeholder for your reranker implementation

class LegalDocumentWorkflow:
    """
    Workflow for retrieving and analyzing legal documents for case preparation.

    Specification:
    - Initialize a LangSwarm agent.
    - Set up a legal retriever and a legal domain-specific reranker.
    - Fetch, process, and load legal data into the retriever backend.
    - Handle user queries and provide ranked legal document results.
    - Implement fallback for cases where no documents are retrieved or reranked.
    """

    def __init__(self, retriever_config, reranker_config, legal_data):
        """
        Initialize the legal document assistant workflow.

        Args:
            retriever_config (dict): Configuration for setting up the legal retriever.
            reranker_config (dict): Configuration for the legal domain-specific reranker.
            legal_data (list): Data for populating the retriever backend.
        """
        # Initialize LangSwarm agent
        self.agent = LangSwarmAgent()

        # Initialize retriever and reranker
        self.retriever = LegalRetriever(**retriever_config)
        self.reranker = LegalReranker(**reranker_config)

        # Data Fetch and Processing
        self.processed_data = self.process_data(legal_data)

        # Data Load
        self.load_data_to_retriever()

    def process_data(self, raw_data):
        """
        Process the raw data into retriever-compatible formats.

        Args:
            raw_data (list): List of raw legal data.

        Returns:
            list: Processed data ready for loading.
        """
        return [{"text": entry.strip(), "metadata": {"type": "legal"}} for entry in raw_data]

    def load_data_to_retriever(self):
        """
        Load the processed data into the retriever backend.
        """
        self.retriever.add_documents(self.processed_data)

    def run(self, query):
        """
        Execute the legal document assistant workflow.

        Args:
            query (str): User's query.

        Returns:
            str: Response generated by the LangSwarm agent, or None if no results are found.
        """
        # Retrieve documents
        documents = self.retriever.query(query)

        if not documents:
            print("No legal documents retrieved.")
            return None

        # Rerank documents
        reranked_documents = self.reranker.rerank(query, documents)

        if not reranked_documents:
            print("No documents could be reranked.")
            return None

        # Concatenate input for the agent
        context = reranked_documents[0]["text"]
        agent_input = f"Query: {query}\nContext: {context}"

        # Generate and return response
        return self.agent.generate_response(agent_input)


# **Usage Example**

if __name__ == "__main__":
    # Example configuration and legal data
    retriever_config = {"backend": "faiss", "index_path": "legal_index.faiss"}
    reranker_config = {"model_name": "legal-bert"}  # Example domain-specific model
    legal_data = [
        "Case 1: LangSwarm v. AI Tech Corp.",
        "Case 2: Copyright law in the age of AI.",
        "Case 3: Privacy and data protection regulations."
    ]

    # Initialize workflow
    legal_assistant = LegalDocumentWorkflow(retriever_config, reranker_config, legal_data)

    # User query
    user_query = "What are the key points in LangSwarm v. AI Tech Corp.?"

    # Run the workflow
    response = legal_assistant.run(user_query)

    if response:
        print("Legal Assistant Response:", response)
    else:
        print("No relevant response could be generated.")



--------------------------------------------------------------------------------
File: memory/templates/chatbot.py
--------------------------------------------------------------------------------

from langswarm.agent import LangSwarmAgent
from retrievers import HybridRetriever  # Placeholder for your retriever implementation
from rerankers import SemanticReranker  # Placeholder for your reranker implementation

class ChatbotWorkflow:
    """
    Chatbot workflow that retrieves context from a knowledge base, reranks responses,
    and generates a reply using a LangSwarm agent.

    Specification:
    - Initialize a LangSwarm agent.
    - Set up the retriever(s) and reranker(s).
    - Fetch, process, and load data into the retriever backend.
    - Handle user queries and generate a contextual response.
    - Implement fallback for cases where no documents are retrieved or reranked.
    """

    def __init__(self, retriever_config, reranker_config, data_files):
        """
        Initialize the chatbot workflow.

        Args:
            retriever_config (dict): Configuration for setting up the retriever.
            reranker_config (dict): Configuration for setting up the reranker.
            data_files (list): List of paths to data files to be loaded.
        """
        # Initialize LangSwarm agent
        self.agent = LangSwarmAgent()

        # Initialize retriever and reranker
        self.retriever = HybridRetriever(**retriever_config)
        self.reranker = SemanticReranker(**reranker_config)

        # Data Fetch and Processing
        self.data = self.fetch_data(data_files)
        self.processed_data = self.process_data(self.data)

        # Data Load
        self.load_data_to_retriever(self.processed_data)

    def fetch_data(self, data_files):
        """
        Fetch data from files.

        Args:
            data_files (list): List of file paths.

        Returns:
            list: Raw data fetched from the files.
        """
        raw_data = []
        for file in data_files:
            with open(file, 'r') as f:
                raw_data.extend(f.readlines())
        return raw_data

    def process_data(self, raw_data):
        """
        Process the raw data into the format required by the retriever.

        Args:
            raw_data (list): List of raw data.

        Returns:
            list: Processed data in retriever-compatible format.
        """
        processed_data = []
        for line in raw_data:
            processed_data.append({"text": line.strip(), "metadata": {}})
        return processed_data

    def load_data_to_retriever(self, processed_data):
        """
        Load the processed data into the retriever backend.

        Args:
            processed_data (list): Data ready to be loaded.
        """
        self.retriever.add_documents(processed_data)

    def run(self, query):
        """
        Execute the chatbot workflow.

        Args:
            query (str): User's query.

        Returns:
            str: Chatbot response generated by the LangSwarm agent, or None if no results are found.
        """
        # Retrieve and rerank
        documents = self.retriever.query(query)
        if not documents:
            print("No documents retrieved.")
            return None

        reranked_documents = self.reranker.rerank(query, documents)
        if not reranked_documents:
            print("No documents could be reranked.")
            return None

        # Concatenate input for the agent
        context = reranked_documents[0]["text"]
        agent_input = f"Query: {query}\nContext: {context}"

        # Generate and return response
        return self.agent.generate_response(agent_input)


# **Usage Example**

if __name__ == "__main__":
    # Example configuration and data files
    retriever_config = {"backend": "pinecone", "api_key": "your-api-key"}
    reranker_config = {"model_name": "all-MiniLM-L6-v2"}
    data_files = ["knowledge_base.txt"]

    # Initialize workflow
    chatbot = ChatbotWorkflow(retriever_config, reranker_config, data_files)

    # User query
    user_query = "What is LangSwarm?"

    # Run the workflow
    response = chatbot.run(user_query)

    if response:
        print("Chatbot Response:", response)
    else:
        print("No relevant response could be generated.")



--------------------------------------------------------------------------------
File: memory/templates/federated.py
--------------------------------------------------------------------------------

from langswarm.agent import LangSwarmAgent
from retrievers import DistributedRetriever  # Placeholder for your retriever implementation

class FederatedKnowledgeWorkflow:
    """
    Workflow for retrieving data from distributed knowledge repositories.

    Specification:
    - Initialize a LangSwarm agent.
    - Set up multiple retrievers for distributed knowledge sources.
    - Fetch, process, and load data into respective retriever backends.
    - Handle user queries and aggregate results from all sources.
    - Implement fallback for cases where no documents are retrieved.
    """

    def __init__(self, retriever_configs, data_sources):
        """
        Initialize the federated knowledge workflow.

        Args:
            retriever_configs (list): List of configurations for setting up distributed retrievers.
            data_sources (list): List of data sources (files or database connections).
        """
        # Initialize LangSwarm agent
        self.agent = LangSwarmAgent()

        # Initialize distributed retrievers
        self.retrievers = [DistributedRetriever(**config) for config in retriever_configs]

        # Data Fetch and Processing
        self.data = self.fetch_data(data_sources)
        self.processed_data = self.process_data(self.data)

        # Data Load
        self.load_data_to_retrievers()

    def fetch_data(self, data_sources):
        """
        Fetch data from distributed sources.

        Args:
            data_sources (list): List of file paths or database queries.

        Returns:
            dict: Raw data fetched from each source, keyed by retriever index.
        """
        raw_data = {}
        for i, source in enumerate(data_sources):
            if isinstance(source, str):  # File source
                with open(source, 'r') as f:
                    raw_data[i] = f.readlines()
            elif callable(source):  # Database query function
                raw_data[i] = source()  # Assume the function returns a list of records
        return raw_data

    def process_data(self, raw_data):
        """
        Process the raw data into retriever-compatible formats.

        Args:
            raw_data (dict): Raw data keyed by retriever index.

        Returns:
            dict: Processed data ready for loading, keyed by retriever index.
        """
        processed_data = {}
        for idx, data in raw_data.items():
            processed_data[idx] = [{"text": entry.strip(), "metadata": {}} for entry in data]
        return processed_data

    def load_data_to_retrievers(self):
        """
        Load the processed data into respective retriever backends.
        """
        for idx, retriever in enumerate(self.retrievers):
            retriever.add_documents(self.processed_data[idx])

    def run(self, query):
        """
        Execute the federated knowledge access workflow.

        Args:
            query (str): User's query.

        Returns:
            str: Aggregated response generated by the LangSwarm agent, or None if no results are found.
        """
        # Retrieve documents from all sources
        all_documents = []
        for retriever in self.retrievers:
            documents = retriever.query(query)
            if not documents:
                print(f"No documents retrieved from retriever {retriever}.")
                continue
            all_documents.extend(documents)

        if not all_documents:
            print("No documents retrieved from any distributed sources.")
            return None

        # Concatenate input for the agent
        context = "\n".join([doc["text"] for doc in all_documents[:5]])  # Top 5 results
        agent_input = f"Query: {query}\nAggregated Context:\n{context}"

        # Generate and return response
        return self.agent.generate_response(agent_input)


# **Usage Example**

if __name__ == "__main__":
    # Example configurations and data sources
    retriever_configs = [
        {"backend": "pinecone", "api_key": "your-api-key"},
        {"backend": "elasticsearch", "connection_string": "http://localhost:9200"}
    ]
    data_sources = [
        "distributed_knowledge_1.txt",
        lambda: ["Record 1 from DB", "Record 2 from DB"]
    ]

    # Initialize workflow
    federated_knowledge = FederatedKnowledgeWorkflow(retriever_configs, data_sources)

    # User query
    user_query = "What are the current trends in AI?"

    # Run the workflow
    response = federated_knowledge.run(user_query)

    if response:
        print("Federated Knowledge Response:", response)
    else:
        print("No relevant response could be generated.")



--------------------------------------------------------------------------------
File: memory/templates/multilingual.py
--------------------------------------------------------------------------------

from langswarm.agent import LangSwarmAgent
from retrievers import MultilingualRetriever  # Placeholder for your retriever implementation
from translators import Translator  # Placeholder for your translation implementation

class MultilingualKnowledgeWorkflow:
    """
    Workflow for retrieving and translating knowledge across multiple languages.

    Specification:
    - Initialize a LangSwarm agent.
    - Set up a multilingual retriever and translator.
    - Fetch, process, and load multilingual data into the retriever backend.
    - Handle user queries and provide translated results.
    - Implement fallback for cases where no documents are retrieved or translation fails.
    """

    def __init__(self, retriever_config, translator_config, multilingual_data):
        """
        Initialize the multilingual knowledge retrieval workflow.

        Args:
            retriever_config (dict): Configuration for setting up the multilingual retriever.
            translator_config (dict): Configuration for setting up the translator.
            multilingual_data (list): Data for populating the retriever backend.
        """
        # Initialize LangSwarm agent
        self.agent = LangSwarmAgent()

        # Initialize retriever and translator
        self.retriever = MultilingualRetriever(**retriever_config)
        self.translator = Translator(**translator_config)

        # Data Fetch and Processing
        self.processed_data = self.process_data(multilingual_data)

        # Data Load
        self.load_data_to_retriever()

    def process_data(self, raw_data):
        """
        Process the raw data into retriever-compatible formats.

        Args:
            raw_data (list): List of raw multilingual data.

        Returns:
            list: Processed data ready for loading.
        """
        return [{"text": entry["text"].strip(), "metadata": {"language": entry["language"]}} for entry in raw_data]

    def load_data_to_retriever(self):
        """
        Load the processed data into the retriever backend.
        """
        self.retriever.add_documents(self.processed_data)

    def run(self, query, target_language):
        """
        Execute the multilingual knowledge retrieval workflow.

        Args:
            query (str): User's query.
            target_language (str): Target language for translation.

        Returns:
            str: Translated knowledge response generated by the LangSwarm agent, or None if no results are found.
        """
        # Retrieve documents
        documents = self.retriever.query(query)

        if not documents:
            print("No multilingual documents retrieved.")
            return None

        # Translate the top document
        top_document = documents[0]
        try:
            translated_text = self.translator.translate(top_document["text"], target_language)
        except Exception as e:
            print(f"Translation failed: {e}")
            return None

        # Concatenate input for the agent
        agent_input = f"Query: {query}\nTranslated Context: {translated_text}"

        # Generate and return response
        return self.agent.generate_response(agent_input)


# **Usage Example**

if __name__ == "__main__":
    # Example configuration and multilingual data
    retriever_config = {"backend": "pinecone", "api_key": "your-api-key"}
    translator_config = {"service": "google", "api_key": "translator-api-key"}
    multilingual_data = [
        {"text": "Qu es LangSwarm?", "language": "es"},
        {"text": "LangSwarm nedir?", "language": "tr"},
        {"text": "Qu'est-ce que LangSwarm ?", "language": "fr"},
        {"text": "What is LangSwarm?", "language": "en"}
    ]

    # Initialize workflow
    multilingual_knowledge = MultilingualKnowledgeWorkflow(retriever_config, translator_config, multilingual_data)

    # User query
    user_query = "What is LangSwarm?"
    target_language = "fr"  # French

    # Run the workflow
    response = multilingual_knowledge.run(user_query, target_language)

    if response:
        print("Multilingual Knowledge Response:", response)
    else:
        print("No relevant response could be generated.")



--------------------------------------------------------------------------------
File: memory/templates/hybrid.py
--------------------------------------------------------------------------------

from langswarm.agent import LangSwarmAgent
from retrievers import HybridRetriever, BM25Retriever  # Placeholder for retriever implementations
from rerankers import CombinedRerankingWorkflow  # Placeholder for reranking workflow implementation

class HybridRetrievalRerankingWorkflow:
    """
    Workflow for combining hybrid retrieval (dense + sparse) and reranking strategies.

    Specification:
    - Retrieve documents using both dense and sparse retrievers.
    - Combine results and apply multi-agent reranking for improved ranking.
    - Generate responses based on the top-ranked results.
    """

    def __init__(self, dense_config, sparse_config, reranker_configs, documents):
        """
        Initialize the workflow.

        Args:
            dense_config (dict): Configuration for the dense retriever.
            sparse_config (dict): Configuration for the sparse retriever.
            reranker_configs (list): List of reranker configurations.
            documents (list): List of documents to populate retrievers.
        """
        # Initialize LangSwarm agent
        self.agent = LangSwarmAgent()

        # Initialize retrievers
        self.dense_retriever = HybridRetriever(**dense_config)
        self.sparse_retriever = BM25Retriever(documents, **sparse_config)

        # Initialize reranking workflow
        rerankers = [config["reranker"](**config["params"]) for config in reranker_configs]
        self.reranker_workflow = CombinedRerankingWorkflow(rerankers)

        # Data Load
        self.load_data_to_dense_retriever(documents)

    def load_data_to_dense_retriever(self, documents):
        """
        Load documents into the dense retriever backend.

        Args:
            documents (list): List of documents.
        """
        self.dense_retriever.add_documents(documents)

    def run(self, query):
        """
        Execute the hybrid retrieval and reranking workflow.

        Args:
            query (str): User's query.

        Returns:
            str: Response generated by the LangSwarm agent, or None if no results are found.
        """
        # Retrieve using dense and sparse retrievers
        dense_results = self.dense_retriever.query(query)
        sparse_results = self.sparse_retriever.query(query)

        # Combine results
        combined_results = dense_results + sparse_results

        # Rerank combined results
        reranked_results = self.reranker_workflow.run(query, combined_results)

        if not reranked_results:
            print("No documents could be reranked.")
            return None

        # Concatenate input for the agent
        context = reranked_results[0]["text"]
        agent_input = f"Query: {query}\nContext: {context}"

        # Generate and return response
        return self.agent.generate_response(agent_input)



--------------------------------------------------------------------------------
File: memory/templates/research.py
--------------------------------------------------------------------------------

from langswarm.agent import LangSwarmAgent
from retrievers import HybridRetriever  # Placeholder for your retriever implementation
from rerankers import DomainSpecificReranker  # Placeholder for your reranker implementation

class ResearchAssistantWorkflow:
    """
    Workflow for assisting researchers by retrieving and ranking academic papers or articles.

    Specification:
    - Initialize a LangSwarm agent.
    - Set up a hybrid retriever and a domain-specific reranker.
    - Fetch, process, and load research data into the retriever backend.
    - Handle user queries and generate contextual responses.
    - Implement fallback for cases where no documents are retrieved or reranked.
    """

    def __init__(self, retriever_config, reranker_config, research_data):
        """
        Initialize the research assistant workflow.

        Args:
            retriever_config (dict): Configuration for setting up the hybrid retriever.
            reranker_config (dict): Configuration for the domain-specific reranker.
            research_data (list): Data for populating the retriever backend.
        """
        # Initialize LangSwarm agent
        self.agent = LangSwarmAgent()

        # Initialize retriever and reranker
        self.retriever = HybridRetriever(**retriever_config)
        self.reranker = DomainSpecificReranker(**reranker_config)

        # Data Fetch and Processing
        self.processed_data = self.process_data(research_data)

        # Data Load
        self.load_data_to_retriever()

    def process_data(self, raw_data):
        """
        Process the raw data into retriever-compatible formats.

        Args:
            raw_data (list): List of raw research data.

        Returns:
            list: Processed data ready for loading.
        """
        return [{"text": entry.strip(), "metadata": {"source": "research"}} for entry in raw_data]

    def load_data_to_retriever(self):
        """
        Load the processed data into the retriever backend.
        """
        self.retriever.add_documents(self.processed_data)

    def run(self, query):
        """
        Execute the research assistant workflow.

        Args:
            query (str): User's query.

        Returns:
            str: Response generated by the LangSwarm agent, or None if no results are found.
        """
        # Retrieve documents
        documents = self.retriever.query(query)

        if not documents:
            print("No research documents retrieved.")
            return None

        # Rerank documents
        reranked_documents = self.reranker.rerank(query, documents)

        if not reranked_documents:
            print("No documents could be reranked.")
            return None

        # Concatenate input for the agent
        context = reranked_documents[0]["text"]
        agent_input = f"Query: {query}\nContext: {context}"

        # Generate and return response
        return self.agent.generate_response(agent_input)


# **Usage Example**

if __name__ == "__main__":
    # Example configuration and research data
    retriever_config = {"backend": "faiss", "index_path": "research_index.faiss"}
    reranker_config = {"model_name": "scibert"}  # Example domain-specific model
    research_data = [
        "Paper 1: LangSwarm's impact on multi-agent systems.",
        "Paper 2: Applications of reinforcement learning in research.",
        "Paper 3: Advances in large language models for scientific discovery."
    ]

    # Initialize workflow
    research_assistant = ResearchAssistantWorkflow(retriever_config, reranker_config, research_data)

    # User query
    user_query = "What are the latest advancements in reinforcement learning?"

    # Run the workflow
    response = research_assistant.run(user_query)

    if response:
        print("Research Assistant Response:", response)
    else:
        print("No relevant response could be generated.")



--------------------------------------------------------------------------------
File: memory/templates/knowledge_base.py
--------------------------------------------------------------------------------

from langswarm.agent import LangSwarmAgent
from retrievers import HybridRetriever, SQLRetriever  # Placeholder for your retriever implementations
from rerankers import CombinedReranker  # Placeholder for your reranker implementation

class EnterpriseKnowledgeBaseWorkflow:
    """
    Workflow for retrieving and ranking information from an enterprise knowledge base.

    Specification:
    - Initialize a LangSwarm agent.
    - Set up multiple retrievers and a combined reranker.
    - Fetch, process, and load data into respective retriever backends.
    - Handle user queries and generate contextual responses.
    - Implement fallback for cases where no documents are retrieved or reranked.
    """

    def __init__(self, retriever_configs, reranker_config, data_sources):
        """
        Initialize the enterprise knowledge base workflow.

        Args:
            retriever_configs (list): List of configurations for each retriever.
            reranker_config (dict): Configuration for the combined reranker.
            data_sources (list): List of data sources (files or database connections).
        """
        # Initialize LangSwarm agent
        self.agent = LangSwarmAgent()

        # Initialize retrievers
        self.retrievers = [HybridRetriever(**config) if config["type"] == "hybrid" else SQLRetriever(**config)
                           for config in retriever_configs]

        # Initialize reranker
        self.reranker = CombinedReranker(**reranker_config)

        # Data Fetch and Processing
        self.data = self.fetch_data(data_sources)
        self.processed_data = self.process_data(self.data)

        # Data Load
        self.load_data_to_retrievers(self.processed_data)

    def fetch_data(self, data_sources):
        """
        Fetch data from various sources.

        Args:
            data_sources (list): List of file paths or database queries.

        Returns:
            dict: Raw data fetched from each source, keyed by retriever index.
        """
        raw_data = {}
        for i, source in enumerate(data_sources):
            if isinstance(source, str):  # File source
                with open(source, 'r') as f:
                    raw_data[i] = f.readlines()
            elif callable(source):  # Database query function
                raw_data[i] = source()  # Assume the function returns a list of records
        return raw_data

    def process_data(self, raw_data):
        """
        Process the raw data into retriever-compatible formats.

        Args:
            raw_data (dict): Raw data keyed by retriever index.

        Returns:
            dict: Processed data ready for loading, keyed by retriever index.
        """
        processed_data = {}
        for idx, data in raw_data.items():
            processed_data[idx] = [{"text": entry.strip(), "metadata": {}} for entry in data]
        return processed_data

    def load_data_to_retrievers(self, processed_data):
        """
        Load the processed data into respective retriever backends.

        Args:
            processed_data (dict): Processed data keyed by retriever index.
        """
        for idx, retriever in enumerate(self.retrievers):
            retriever.add_documents(processed_data[idx])

    def run(self, query):
        """
        Execute the knowledge base workflow.

        Args:
            query (str): User's query.

        Returns:
            str: Response generated by the LangSwarm agent, or None if no results are found.
        """
        # Retrieve documents from all sources
        all_documents = []
        for retriever in self.retrievers:
            documents = retriever.query(query)
            if not documents:
                print(f"No documents retrieved from retriever {retriever}.")
                continue
            all_documents.extend(documents)

        if not all_documents:
            print("No documents retrieved from any source.")
            return None

        # Rerank documents
        reranked_documents = self.reranker.rerank(query, all_documents)
        if not reranked_documents:
            print("No documents could be reranked.")
            return None

        # Concatenate input for the agent
        context = reranked_documents[0]["text"]
        agent_input = f"Query: {query}\nContext: {context}"

        # Generate and return response
        return self.agent.generate_response(agent_input)


# **Usage Example**

if __name__ == "__main__":
    # Example configurations and data sources
    retriever_configs = [
        {"type": "hybrid", "backend": "pinecone", "api_key": "your-api-key"},
        {"type": "sql", "connection_string": "sqlite:///enterprise_knowledge.db"}
    ]
    reranker_config = {"weights": [0.7, 0.3]}  # Example configuration for weighted reranking
    data_sources = ["knowledge_base.txt", lambda: ["Record 1 from DB", "Record 2 from DB"]]

    # Initialize workflow
    knowledge_base = EnterpriseKnowledgeBaseWorkflow(retriever_configs, reranker_config, data_sources)

    # User query
    user_query = "What is our enterprise's vision?"

    # Run the workflow
    response = knowledge_base.run(user_query)

    if response:
        print("Knowledge Base Response:", response)
    else:
        print("No relevant response could be generated.")



--------------------------------------------------------------------------------
File: memory/templates/temporal.py
--------------------------------------------------------------------------------

from langswarm.agent import LangSwarmAgent
from retrievers import TemporalRetriever  # Placeholder for your retriever implementation

class TemporalRetrievalWorkflow:
    """
    Workflow for retrieving documents based on temporal constraints.

    Specification:
    - Initialize a LangSwarm agent.
    - Set up a temporal retriever.
    - Fetch, process, and load temporally annotated data into the retriever backend.
    - Handle user queries and apply temporal filters.
    - Implement fallback for cases where no documents are retrieved or match temporal constraints.
    """

    def __init__(self, retriever_config, temporal_data):
        """
        Initialize the temporal retrieval workflow.

        Args:
            retriever_config (dict): Configuration for setting up the temporal retriever.
            temporal_data (list): Data with temporal metadata for populating the retriever backend.
        """
        # Initialize LangSwarm agent
        self.agent = LangSwarmAgent()

        # Initialize retriever
        self.retriever = TemporalRetriever(**retriever_config)

        # Data Fetch and Processing
        self.processed_data = self.process_data(temporal_data)

        # Data Load
        self.load_data_to_retriever()

    def process_data(self, raw_data):
        """
        Process the raw data into retriever-compatible formats with temporal metadata.

        Args:
            raw_data (list): List of raw temporal data.

        Returns:
            list: Processed data ready for loading.
        """
        return [
            {"text": entry["text"].strip(), "metadata": {"timestamp": entry["timestamp"]}}
            for entry in raw_data
        ]

    def load_data_to_retriever(self):
        """
        Load the processed data into the retriever backend.
        """
        self.retriever.add_documents(self.processed_data)

    def run(self, query, start_date, end_date):
        """
        Execute the temporal retrieval workflow.

        Args:
            query (str): User's query.
            start_date (str): Start date for the temporal filter (ISO format).
            end_date (str): End date for the temporal filter (ISO format).

        Returns:
            str: Response generated by the LangSwarm agent, or None if no results match the temporal filter.
        """
        # Retrieve documents
        documents = self.retriever.query(query)

        if not documents:
            print("No documents retrieved.")
            return None

        # Apply temporal filter
        filtered_documents = [
            doc
            for doc in documents
            if start_date <= doc["metadata"]["timestamp"] <= end_date
        ]

        if not filtered_documents:
            print("No documents match the temporal filter.")
            return None

        # Concatenate input for the agent
        context = filtered_documents[0]["text"]
        agent_input = f"Query: {query}\nContext: {context}"

        # Generate and return response
        return self.agent.generate_response(agent_input)


# **Usage Example**

if __name__ == "__main__":
    # Example configuration and temporal data
    retriever_config = {"backend": "faiss", "index_path": "temporal_index.faiss"}
    temporal_data = [
        {"text": "Event 1: LangSwarm release.", "timestamp": "2025-01-01"},
        {"text": "Event 2: AI conference keynote.", "timestamp": "2025-02-15"},
        {"text": "Event 3: Research breakthrough.", "timestamp": "2025-03-10"}
    ]

    # Initialize workflow
    temporal_retrieval = TemporalRetrievalWorkflow(retriever_config, temporal_data)

    # User query
    user_query = "What events happened in early 2025?"
    start_date = "2025-01-01"
    end_date = "2025-02-28"

    # Run the workflow
    response = temporal_retrieval.run(user_query, start_date, end_date)

    if response:
        print("Temporal Retrieval Response:", response)
    else:
        print("No relevant response could be generated.")



--------------------------------------------------------------------------------
File: memory/templates/recommender.py
--------------------------------------------------------------------------------

from langswarm.agent import LangSwarmAgent
from retrievers import EmbeddingRetriever  # Placeholder for your retriever implementation
from rerankers import CombinedReranker  # Placeholder for your reranker implementation

class RecommendationSystemWorkflow:
    """
    Workflow for providing personalized recommendations based on user preferences.

    Specification:
    - Initialize a LangSwarm agent.
    - Set up an embedding-based retriever and a combined reranker.
    - Fetch, process, and load recommendation data into the retriever backend.
    - Handle user queries and generate personalized recommendations.
    - Implement fallback for cases where no documents are retrieved or reranked.
    """

    def __init__(self, retriever_config, reranker_config, item_data):
        """
        Initialize the recommendation system workflow.

        Args:
            retriever_config (dict): Configuration for setting up the embedding-based retriever.
            reranker_config (dict): Configuration for the combined reranker.
            item_data (list): Data for populating the retriever backend.
        """
        # Initialize LangSwarm agent
        self.agent = LangSwarmAgent()

        # Initialize retriever and reranker
        self.retriever = EmbeddingRetriever(**retriever_config)
        self.reranker = CombinedReranker(**reranker_config)

        # Data Fetch and Processing
        self.processed_data = self.process_data(item_data)

        # Data Load
        self.load_data_to_retriever()

    def process_data(self, raw_data):
        """
        Process the raw data into retriever-compatible formats.

        Args:
            raw_data (list): List of raw item data.

        Returns:
            list: Processed data ready for loading.
        """
        return [{"text": entry.strip(), "metadata": {"type": "item"}} for entry in raw_data]

    def load_data_to_retriever(self):
        """
        Load the processed data into the retriever backend.
        """
        self.retriever.add_documents(self.processed_data)

    def run(self, user_profile):
        """
        Execute the recommendation workflow.

        Args:
            user_profile (str): User's preferences or query.

        Returns:
            str: Personalized recommendations generated by the LangSwarm agent, or None if no results are found.
        """
        # Retrieve similar items
        documents = self.retriever.query(user_profile)

        if not documents:
            print("No recommendations retrieved.")
            return None

        # Rerank items
        reranked_documents = self.reranker.rerank(user_profile, documents)

        if not reranked_documents:
            print("No recommendations could be reranked.")
            return None

        # Concatenate input for the agent
        context = "\n".join([doc["text"] for doc in reranked_documents[:3]])  # Top 3 recommendations
        agent_input = f"User Profile: {user_profile}\nRecommendations:\n{context}"

        # Generate and return response
        return self.agent.generate_response(agent_input)


# **Usage Example**

if __name__ == "__main__":
    # Example configuration and item data
    retriever_config = {"backend": "pinecone", "api_key": "your-api-key"}
    reranker_config = {"weights": [0.8, 0.2]}  # Example weights for relevance and popularity
    item_data = [
        "Item 1: LangSwarm integration toolkit.",
        "Item 2: Advanced AI solutions guide.",
        "Item 3: Personalized recommendation strategies.",
        "Item 4: Multi-agent system development resources."
    ]

    # Initialize workflow
    recommendation_system = RecommendationSystemWorkflow(retriever_config, reranker_config, item_data)

    # User preferences
    user_profile = "Looking for tools to integrate AI and multi-agent systems."

    # Run the workflow
    response = recommendation_system.run(user_profile)

    if response:
        print("Personalized Recommendations:", response)
    else:
        print("No relevant recommendations could be generated.")


